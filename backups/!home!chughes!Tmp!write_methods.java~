java/org/apache/hadoop/mapred/lib/LazyOutputFormat.java:107:    public void write(K key, V value) throws IOException {
java/org/apache/hadoop/mapred/lib/LazyOutputFormat.java-108-      if (rawWriter == null) {
java/org/apache/hadoop/mapred/lib/LazyOutputFormat.java-109-        createRecordWriter();
java/org/apache/hadoop/mapred/lib/LazyOutputFormat.java-110-      }
java/org/apache/hadoop/mapred/lib/LazyOutputFormat.java-111-      super.write(key, value);
java/org/apache/hadoop/mapred/lib/LazyOutputFormat.java-112-    }
java/org/apache/hadoop/mapred/lib/LazyOutputFormat.java-113-
java/org/apache/hadoop/mapred/lib/LazyOutputFormat.java-114-    @SuppressWarnings("unchecked")
java/org/apache/hadoop/mapred/lib/LazyOutputFormat.java-115-    private void createRecordWriter() throws IOException {
java/org/apache/hadoop/mapred/lib/LazyOutputFormat.java-116-      FileSystem fs = FileSystem.get(job);
java/org/apache/hadoop/mapred/lib/LazyOutputFormat.java-117-      rawWriter = of.getRecordWriter(fs, job, name, progress);
java/org/apache/hadoop/mapred/lib/LazyOutputFormat.java-118-    }  
java/org/apache/hadoop/mapred/lib/LazyOutputFormat.java-119-  }
java/org/apache/hadoop/mapred/lib/LazyOutputFormat.java-120-}
--
java/org/apache/hadoop/mapred/lib/FilterOutputFormat.java:91:    public void write(K key, V value) throws IOException {
java/org/apache/hadoop/mapred/lib/FilterOutputFormat.java-92-      getRawWriter().write(key, value);
java/org/apache/hadoop/mapred/lib/FilterOutputFormat.java-93-    }
java/org/apache/hadoop/mapred/lib/FilterOutputFormat.java-94-    
java/org/apache/hadoop/mapred/lib/FilterOutputFormat.java-95-    private RecordWriter<K,V> getRawWriter() throws IOException {
java/org/apache/hadoop/mapred/lib/FilterOutputFormat.java-96-      if (rawWriter == null) {
java/org/apache/hadoop/mapred/lib/FilterOutputFormat.java-97-        throw new IOException ("Record Writer not set for FilterRecordWriter");
java/org/apache/hadoop/mapred/lib/FilterOutputFormat.java-98-      }
java/org/apache/hadoop/mapred/lib/FilterOutputFormat.java-99-      return rawWriter;
java/org/apache/hadoop/mapred/lib/FilterOutputFormat.java-100-    }
java/org/apache/hadoop/mapred/lib/FilterOutputFormat.java-101-  }
java/org/apache/hadoop/mapred/lib/FilterOutputFormat.java-102-
java/org/apache/hadoop/mapred/lib/FilterOutputFormat.java-103-}
--
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java:126:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-127-    Text.writeString(out, inputSplitClass.getName());
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-128-    inputSplit.write(out);
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-129-    Text.writeString(out, inputFormatClass.getName());
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-130-    Text.writeString(out, mapperClass.getName());
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-131-  }
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-132-
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-133-  public Configuration getConf() {
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-134-    return conf;
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-135-  }
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-136-
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-137-  public void setConf(Configuration conf) {
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-138-    this.conf = conf;
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-139-  }
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-140-
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-141-  @Override
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-142-  public String toString() {
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-143-    return inputSplit.toString();
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-144-  }
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-145-
java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java-146-}
--
java/org/apache/hadoop/mapred/lib/NullOutputFormat.java:40:        public void write(K key, V value) { }
java/org/apache/hadoop/mapred/lib/NullOutputFormat.java-41-        public void close(Reporter reporter) { }
java/org/apache/hadoop/mapred/lib/NullOutputFormat.java-42-      };
java/org/apache/hadoop/mapred/lib/NullOutputFormat.java-43-  }
java/org/apache/hadoop/mapred/lib/NullOutputFormat.java-44-  
java/org/apache/hadoop/mapred/lib/NullOutputFormat.java-45-  public void checkOutputSpecs(FileSystem ignored, JobConf job) { }
java/org/apache/hadoop/mapred/lib/NullOutputFormat.java-46-}
--
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java:87:      public void write(K key, V value) throws IOException {
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-88-
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-89-        // get the file name based on the key
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-90-        String keyBasedPath = generateFileNameForKeyValue(key, value, myName);
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-91-
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-92-        // get the file name based on the input file name
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-93-        String finalPath = getInputFileBasedOutputFileName(myJob, keyBasedPath);
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-94-
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-95-        // get the actual key
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-96-        K actualKey = generateActualKey(key, value);
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-97-        V actualValue = generateActualValue(key, value);
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-98-
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-99-        RecordWriter<K, V> rw = this.recordWriters.get(finalPath);
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-100-        if (rw == null) {
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-101-          // if we don't have the record writer yet for the final path, create
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-102-          // one
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-103-          // and add it to the cache
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-104-          rw = getBaseRecordWriter(myFS, myJob, finalPath, myProgressable);
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-105-          this.recordWriters.put(finalPath, rw);
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-106-        }
java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java-107-        rw.write(actualKey, actualValue);
--
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java:456:    public void write(Object key, Object value) throws IOException {
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-457-      reporter.incrCounter(COUNTERS_GROUP, counterName, 1);
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-458-      writer.write(key, value);
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-459-    }
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-460-
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-461-    public void close(Reporter reporter) throws IOException {
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-462-      writer.close(reporter);
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-463-    }
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-464-  }
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-465-
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-466-  /**
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-467-   * Gets the output collector for a named output.
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-468-   * <p/>
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-469-   *
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-470-   * @param namedOutput the named output name
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-471-   * @param reporter    the reporter
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-472-   * @return the output collector for the given named output
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-473-   * @throws IOException thrown if output collector could not be created
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-474-   */
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-475-  @SuppressWarnings({"unchecked"})
java/org/apache/hadoop/mapred/lib/MultipleOutputs.java-476-  public OutputCollector getCollector(String namedOutput, Reporter reporter)
--
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java:55:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java-56-    out.writeBoolean(reset);
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java-57-    out.writeInt(events.length);
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java-58-    for (TaskCompletionEvent event : events) {
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java-59-      event.write(out);
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java-60-    }
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java-61-  }
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java-62-
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java-63-  public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java-64-    reset = in.readBoolean();
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java-65-    events = new TaskCompletionEvent[in.readInt()];
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java-66-    for (int i = 0; i < events.length; ++i) {
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java-67-      events[i] = new TaskCompletionEvent();
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java-68-      events[i].readFields(in);
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java-69-    }
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java-70-  }
java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java-71-}
--
java/org/apache/hadoop/mapred/MapTaskStatus.java:93:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/MapTaskStatus.java-94-    super.write(out);
java/org/apache/hadoop/mapred/MapTaskStatus.java-95-    out.writeLong(mapFinishTime);
java/org/apache/hadoop/mapred/MapTaskStatus.java-96-  }
java/org/apache/hadoop/mapred/MapTaskStatus.java-97-
java/org/apache/hadoop/mapred/MapTaskStatus.java-98-  @Override
java/org/apache/hadoop/mapred/MapTaskStatus.java-99-  public void addFetchFailedMap(TaskAttemptID mapTaskId) {
java/org/apache/hadoop/mapred/MapTaskStatus.java-100-    throw new UnsupportedOperationException
java/org/apache/hadoop/mapred/MapTaskStatus.java-101-                ("addFetchFailedMap() not supported for MapTask");
java/org/apache/hadoop/mapred/MapTaskStatus.java-102-  }
java/org/apache/hadoop/mapred/MapTaskStatus.java-103-
java/org/apache/hadoop/mapred/MapTaskStatus.java-104-}
--
java/org/apache/hadoop/mapred/Counters.java:189:    public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/Counters.java-190-      realCounter.write(out);
java/org/apache/hadoop/mapred/Counters.java-191-    }
java/org/apache/hadoop/mapred/Counters.java-192-
java/org/apache/hadoop/mapred/Counters.java-193-    @Override
java/org/apache/hadoop/mapred/Counters.java-194-    public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapred/Counters.java-195-      realCounter.readFields(in);
java/org/apache/hadoop/mapred/Counters.java-196-    }
java/org/apache/hadoop/mapred/Counters.java-197-
java/org/apache/hadoop/mapred/Counters.java-198-    /**
java/org/apache/hadoop/mapred/Counters.java-199-     * Returns the compact stringified version of the counter in the format
java/org/apache/hadoop/mapred/Counters.java-200-     * [(actual-name)(display-name)(value)]
java/org/apache/hadoop/mapred/Counters.java-201-     * @return the stringified result
java/org/apache/hadoop/mapred/Counters.java-202-     */
java/org/apache/hadoop/mapred/Counters.java-203-    public String makeEscapedCompactString() {
java/org/apache/hadoop/mapred/Counters.java-204-      return toEscapedCompactString(realCounter);
java/org/apache/hadoop/mapred/Counters.java-205-    }
java/org/apache/hadoop/mapred/Counters.java-206-
java/org/apache/hadoop/mapred/Counters.java-207-    /**
java/org/apache/hadoop/mapred/Counters.java-208-     * Checks for (content) equality of two (basic) counters
java/org/apache/hadoop/mapred/Counters.java-209-     * @param counter to compare
--
java/org/apache/hadoop/mapred/Counters.java:318:    public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/Counters.java-319-     realGroup.write(out); 
java/org/apache/hadoop/mapred/Counters.java-320-    }
java/org/apache/hadoop/mapred/Counters.java-321-
java/org/apache/hadoop/mapred/Counters.java-322-    @Override
java/org/apache/hadoop/mapred/Counters.java-323-    public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapred/Counters.java-324-      realGroup.readFields(in);
java/org/apache/hadoop/mapred/Counters.java-325-    }
java/org/apache/hadoop/mapred/Counters.java-326-
java/org/apache/hadoop/mapred/Counters.java-327-    @Override
java/org/apache/hadoop/mapred/Counters.java-328-    public Iterator<Counter> iterator() {
java/org/apache/hadoop/mapred/Counters.java-329-      return realGroup.iterator();
java/org/apache/hadoop/mapred/Counters.java-330-    }
java/org/apache/hadoop/mapred/Counters.java-331-
java/org/apache/hadoop/mapred/Counters.java-332-    @Override
java/org/apache/hadoop/mapred/Counters.java-333-    public String getName() {
java/org/apache/hadoop/mapred/Counters.java-334-      return realGroup.getName();
java/org/apache/hadoop/mapred/Counters.java-335-    }
java/org/apache/hadoop/mapred/Counters.java-336-
java/org/apache/hadoop/mapred/Counters.java-337-    @Override
java/org/apache/hadoop/mapred/Counters.java-338-    public String getDisplayName() {
--
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java:73:        public void write(K key, V value)
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-74-          throws IOException {
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-75-
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-76-          out.append(key, value);
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-77-        }
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-78-
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-79-        public void close(Reporter reporter) throws IOException { out.close();}
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-80-      };
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-81-  }
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-82-
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-83-  /** Open the output generated by this format. */
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-84-  public static SequenceFile.Reader[] getReaders(Configuration conf, Path dir)
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-85-    throws IOException {
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-86-    FileSystem fs = dir.getFileSystem(conf);
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-87-    Path[] names = FileUtil.stat2Paths(fs.listStatus(dir));
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-88-    
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-89-    // sort names, so that hash partitioning works
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-90-    Arrays.sort(names);
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-91-    
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-92-    SequenceFile.Reader[] parts = new SequenceFile.Reader[names.length];
java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java-93-    for (int i = 0; i < names.length; i++) {
--
java/org/apache/hadoop/mapred/AMFeedback.java:52:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/AMFeedback.java-53-    out.writeBoolean(taskFound);
java/org/apache/hadoop/mapred/AMFeedback.java-54-    out.writeBoolean(preemption);
java/org/apache/hadoop/mapred/AMFeedback.java-55-  }
java/org/apache/hadoop/mapred/AMFeedback.java-56-
java/org/apache/hadoop/mapred/AMFeedback.java-57-  @Override
java/org/apache/hadoop/mapred/AMFeedback.java-58-  public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapred/AMFeedback.java-59-    taskFound = in.readBoolean();
java/org/apache/hadoop/mapred/AMFeedback.java-60-    preemption = in.readBoolean();
java/org/apache/hadoop/mapred/AMFeedback.java-61-  }
java/org/apache/hadoop/mapred/AMFeedback.java-62-
java/org/apache/hadoop/mapred/AMFeedback.java-63-}
--
java/org/apache/hadoop/mapred/SortedRanges.java:199:  public synchronized void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/SortedRanges.java-200-    out.writeLong(indicesCount);
java/org/apache/hadoop/mapred/SortedRanges.java-201-    out.writeInt(ranges.size());
java/org/apache/hadoop/mapred/SortedRanges.java-202-    Iterator<Range> it = ranges.iterator();
java/org/apache/hadoop/mapred/SortedRanges.java-203-    while(it.hasNext()) {
java/org/apache/hadoop/mapred/SortedRanges.java-204-      Range range = it.next();
java/org/apache/hadoop/mapred/SortedRanges.java-205-      range.write(out);
java/org/apache/hadoop/mapred/SortedRanges.java-206-    }
java/org/apache/hadoop/mapred/SortedRanges.java-207-  }
java/org/apache/hadoop/mapred/SortedRanges.java-208-  
java/org/apache/hadoop/mapred/SortedRanges.java-209-  public String toString() {
java/org/apache/hadoop/mapred/SortedRanges.java-210-    StringBuffer sb = new StringBuffer();
java/org/apache/hadoop/mapred/SortedRanges.java-211-    Iterator<Range> it = ranges.iterator();
java/org/apache/hadoop/mapred/SortedRanges.java-212-    while(it.hasNext()) {
java/org/apache/hadoop/mapred/SortedRanges.java-213-      Range range = it.next();
java/org/apache/hadoop/mapred/SortedRanges.java-214-      sb.append(range.toString()+"\n");
java/org/apache/hadoop/mapred/SortedRanges.java-215-    }
java/org/apache/hadoop/mapred/SortedRanges.java-216-    return sb.toString();
java/org/apache/hadoop/mapred/SortedRanges.java-217-  }
java/org/apache/hadoop/mapred/SortedRanges.java-218-  
java/org/apache/hadoop/mapred/SortedRanges.java-219-  /**
--
java/org/apache/hadoop/mapred/SortedRanges.java:300:    public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/SortedRanges.java-301-      out.writeLong(startIndex);
java/org/apache/hadoop/mapred/SortedRanges.java-302-      out.writeLong(length);
java/org/apache/hadoop/mapred/SortedRanges.java-303-    }
java/org/apache/hadoop/mapred/SortedRanges.java-304-    
java/org/apache/hadoop/mapred/SortedRanges.java-305-    public String toString() {
java/org/apache/hadoop/mapred/SortedRanges.java-306-      return startIndex +":" + length;
java/org/apache/hadoop/mapred/SortedRanges.java-307-    }    
java/org/apache/hadoop/mapred/SortedRanges.java-308-  }
java/org/apache/hadoop/mapred/SortedRanges.java-309-  
java/org/apache/hadoop/mapred/SortedRanges.java-310-  /**
java/org/apache/hadoop/mapred/SortedRanges.java-311-   * Index Iterator which skips the stored ranges.
java/org/apache/hadoop/mapred/SortedRanges.java-312-   */
java/org/apache/hadoop/mapred/SortedRanges.java-313-  static class SkipRangeIterator implements Iterator<Long> {
java/org/apache/hadoop/mapred/SortedRanges.java-314-    Iterator<Range> rangeIterator;
java/org/apache/hadoop/mapred/SortedRanges.java-315-    Range range = new Range();
java/org/apache/hadoop/mapred/SortedRanges.java-316-    long next = -1;
java/org/apache/hadoop/mapred/SortedRanges.java-317-    
java/org/apache/hadoop/mapred/SortedRanges.java-318-    /**
java/org/apache/hadoop/mapred/SortedRanges.java-319-     * Constructor
java/org/apache/hadoop/mapred/SortedRanges.java-320-     * @param rangeIterator the iterator which gives the ranges.
--
java/org/apache/hadoop/mapred/JobInfo.java:78:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/JobInfo.java-79-    id.write(out);
java/org/apache/hadoop/mapred/JobInfo.java-80-    user.write(out);
java/org/apache/hadoop/mapred/JobInfo.java-81-    WritableUtils.writeString(out, jobSubmitDir.toString());
java/org/apache/hadoop/mapred/JobInfo.java-82-  }
java/org/apache/hadoop/mapred/JobInfo.java-83-}
--
java/org/apache/hadoop/mapred/FileSplit.java:84:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/FileSplit.java-85-    fs.write(out);
java/org/apache/hadoop/mapred/FileSplit.java-86-  }
java/org/apache/hadoop/mapred/FileSplit.java-87-  public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapred/FileSplit.java-88-    fs.readFields(in);
java/org/apache/hadoop/mapred/FileSplit.java-89-  }
java/org/apache/hadoop/mapred/FileSplit.java-90-
java/org/apache/hadoop/mapred/FileSplit.java-91-  public String[] getLocations() throws IOException {
java/org/apache/hadoop/mapred/FileSplit.java-92-    return fs.getLocations();
java/org/apache/hadoop/mapred/FileSplit.java-93-  }
java/org/apache/hadoop/mapred/FileSplit.java-94-  
java/org/apache/hadoop/mapred/FileSplit.java-95-}
--
java/org/apache/hadoop/mapred/ReduceTaskStatus.java:153:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/ReduceTaskStatus.java-154-    super.write(out);
java/org/apache/hadoop/mapred/ReduceTaskStatus.java-155-    out.writeLong(shuffleFinishTime);
java/org/apache/hadoop/mapred/ReduceTaskStatus.java-156-    out.writeLong(sortFinishTime);
java/org/apache/hadoop/mapred/ReduceTaskStatus.java-157-    out.writeInt(failedFetchTasks.size());
java/org/apache/hadoop/mapred/ReduceTaskStatus.java-158-    for (TaskAttemptID taskId : failedFetchTasks) {
java/org/apache/hadoop/mapred/ReduceTaskStatus.java-159-      taskId.write(out);
java/org/apache/hadoop/mapred/ReduceTaskStatus.java-160-    }
java/org/apache/hadoop/mapred/ReduceTaskStatus.java-161-  }
java/org/apache/hadoop/mapred/ReduceTaskStatus.java-162-  
java/org/apache/hadoop/mapred/ReduceTaskStatus.java-163-}
--
java/org/apache/hadoop/mapred/JvmTask.java:48:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/JvmTask.java-49-    out.writeBoolean(shouldDie);
java/org/apache/hadoop/mapred/JvmTask.java-50-    if (t != null) {
java/org/apache/hadoop/mapred/JvmTask.java-51-      out.writeBoolean(true);
java/org/apache/hadoop/mapred/JvmTask.java-52-      out.writeBoolean(t.isMapTask());
java/org/apache/hadoop/mapred/JvmTask.java-53-      t.write(out);
java/org/apache/hadoop/mapred/JvmTask.java-54-    } else {
java/org/apache/hadoop/mapred/JvmTask.java-55-      out.writeBoolean(false);
java/org/apache/hadoop/mapred/JvmTask.java-56-    }
java/org/apache/hadoop/mapred/JvmTask.java-57-  }
java/org/apache/hadoop/mapred/JvmTask.java-58-  public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapred/JvmTask.java-59-    shouldDie = in.readBoolean();
java/org/apache/hadoop/mapred/JvmTask.java-60-    boolean taskComing = in.readBoolean();
java/org/apache/hadoop/mapred/JvmTask.java-61-    if (taskComing) {
java/org/apache/hadoop/mapred/JvmTask.java-62-      boolean isMap = in.readBoolean();
java/org/apache/hadoop/mapred/JvmTask.java-63-      if (isMap) {
java/org/apache/hadoop/mapred/JvmTask.java-64-        t = new MapTask();
java/org/apache/hadoop/mapred/JvmTask.java-65-      } else {
java/org/apache/hadoop/mapred/JvmTask.java-66-        t = new ReduceTask();
java/org/apache/hadoop/mapred/JvmTask.java-67-      }
java/org/apache/hadoop/mapred/JvmTask.java-68-      t.readFields(in);
--
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java:142:        public void write(BytesWritable bkey, BytesWritable bvalue)
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-143-          throws IOException {
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-144-
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-145-          wvaluebytes.reset(bvalue);
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-146-          out.appendRaw(bkey.getBytes(), 0, bkey.getLength(), wvaluebytes);
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-147-          wvaluebytes.reset(null);
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-148-        }
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-149-
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-150-        public void close(Reporter reporter) throws IOException { 
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-151-          out.close();
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-152-        }
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-153-
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-154-      };
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-155-
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-156-  }
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-157-
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-158-  @Override 
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-159-  public void checkOutputSpecs(FileSystem ignored, JobConf job) 
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-160-            throws IOException {
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-161-    super.checkOutputSpecs(ignored, job);
java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java-162-    if (getCompressOutput(job) && 
--
java/org/apache/hadoop/mapred/JVMId.java:116:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/JVMId.java-117-    super.write(out);
java/org/apache/hadoop/mapred/JVMId.java-118-    jobId.write(out);
java/org/apache/hadoop/mapred/JVMId.java-119-    out.writeBoolean(isMap);
java/org/apache/hadoop/mapred/JVMId.java-120-  }
java/org/apache/hadoop/mapred/JVMId.java-121-  
java/org/apache/hadoop/mapred/JVMId.java-122-  /** Construct a JVMId object from given string 
java/org/apache/hadoop/mapred/JVMId.java-123-   * @return constructed JVMId object or null if the given String is null
java/org/apache/hadoop/mapred/JVMId.java-124-   * @throws IllegalArgumentException if the given string is malformed
java/org/apache/hadoop/mapred/JVMId.java-125-   */
java/org/apache/hadoop/mapred/JVMId.java-126-  public static JVMId forName(String str) 
java/org/apache/hadoop/mapred/JVMId.java-127-    throws IllegalArgumentException {
java/org/apache/hadoop/mapred/JVMId.java-128-    if(str == null)
java/org/apache/hadoop/mapred/JVMId.java-129-      return null;
java/org/apache/hadoop/mapred/JVMId.java-130-    try {
java/org/apache/hadoop/mapred/JVMId.java-131-      String[] parts = str.split("_");
java/org/apache/hadoop/mapred/JVMId.java-132-      if(parts.length == 5) {
java/org/apache/hadoop/mapred/JVMId.java-133-        if(parts[0].equals(JVM)) {
java/org/apache/hadoop/mapred/JVMId.java-134-          boolean isMap = false;
java/org/apache/hadoop/mapred/JVMId.java-135-          if(parts[3].equals("m")) isMap = true;
java/org/apache/hadoop/mapred/JVMId.java-136-          else if(parts[3].equals("r")) isMap = false;
--
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java:117:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-118-    WritableUtils.writeVInt(out, splits.length);
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-119-    for (InputSplit s : splits) {
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-120-      Text.writeString(out, s.getClass().getName());
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-121-    }
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-122-    for (InputSplit s : splits) {
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-123-      s.write(out);
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-124-    }
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-125-  }
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-126-
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-127-  /**
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-128-   * {@inheritDoc}
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-129-   * @throws IOException If the child InputSplit cannot be read, typically
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-130-   *                     for faliing access checks.
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-131-   */
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-132-  @SuppressWarnings("unchecked")  // Generic array assignment
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-133-  public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-134-    int card = WritableUtils.readVInt(in);
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-135-    if (splits == null || splits.length != card) {
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-136-      splits = new InputSplit[card];
java/org/apache/hadoop/mapred/join/CompositeInputSplit.java-137-    }
--
java/org/apache/hadoop/mapred/IFileOutputStream.java:86:  public void write(byte[] b, int off, int len) throws IOException {
java/org/apache/hadoop/mapred/IFileOutputStream.java-87-    sum.update(b, off,len);
java/org/apache/hadoop/mapred/IFileOutputStream.java-88-    out.write(b,off,len);
java/org/apache/hadoop/mapred/IFileOutputStream.java-89-  }
java/org/apache/hadoop/mapred/IFileOutputStream.java-90- 
java/org/apache/hadoop/mapred/IFileOutputStream.java-91-  @Override
java/org/apache/hadoop/mapred/IFileOutputStream.java:92:  public void write(int b) throws IOException {
java/org/apache/hadoop/mapred/IFileOutputStream.java-93-    barray[0] = (byte) (b & 0xFF);
java/org/apache/hadoop/mapred/IFileOutputStream.java-94-    write(barray,0,1);
java/org/apache/hadoop/mapred/IFileOutputStream.java-95-  }
java/org/apache/hadoop/mapred/IFileOutputStream.java-96-
java/org/apache/hadoop/mapred/IFileOutputStream.java-97-}
--
java/org/apache/hadoop/mapred/TaskStatus.java:460:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/TaskStatus.java-461-    taskid.write(out);
java/org/apache/hadoop/mapred/TaskStatus.java-462-    out.writeFloat(progress);
java/org/apache/hadoop/mapred/TaskStatus.java-463-    out.writeInt(numSlots);
java/org/apache/hadoop/mapred/TaskStatus.java-464-    WritableUtils.writeEnum(out, runState);
java/org/apache/hadoop/mapred/TaskStatus.java-465-    Text.writeString(out, diagnosticInfo);
java/org/apache/hadoop/mapred/TaskStatus.java-466-    Text.writeString(out, stateString);
java/org/apache/hadoop/mapred/TaskStatus.java-467-    WritableUtils.writeEnum(out, phase);
java/org/apache/hadoop/mapred/TaskStatus.java-468-    out.writeLong(startTime);
java/org/apache/hadoop/mapred/TaskStatus.java-469-    out.writeLong(finishTime);
java/org/apache/hadoop/mapred/TaskStatus.java-470-    out.writeBoolean(includeAllCounters);
java/org/apache/hadoop/mapred/TaskStatus.java-471-    out.writeLong(outputSize);
java/org/apache/hadoop/mapred/TaskStatus.java-472-    counters.write(out);
java/org/apache/hadoop/mapred/TaskStatus.java-473-    nextRecordRange.write(out);
java/org/apache/hadoop/mapred/TaskStatus.java-474-  }
java/org/apache/hadoop/mapred/TaskStatus.java-475-
java/org/apache/hadoop/mapred/TaskStatus.java-476-  public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapred/TaskStatus.java-477-    this.taskid.readFields(in);
java/org/apache/hadoop/mapred/TaskStatus.java-478-    setProgress(in.readFloat());
java/org/apache/hadoop/mapred/TaskStatus.java-479-    this.numSlots = in.readInt();
java/org/apache/hadoop/mapred/TaskStatus.java-480-    this.runState = WritableUtils.readEnum(in, State.class);
--
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java:202:    public void write(byte b[], int off, int len) throws IOException {
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-203-      file.write(b,off,len);
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-204-      out.write(b,off,len);
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-205-    }
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-206-
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java:207:    public void write(int b) throws IOException {
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-208-      file.write(b);
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-209-      out.write(b);
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-210-    }
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-211-
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-212-    public void flush() throws IOException {
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-213-      file.flush();
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-214-      out.flush();
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-215-    }
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-216-
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-217-    public void close() throws IOException {
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-218-      flush();
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-219-      file.close();
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-220-      out.close();
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-221-    }
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-222-  }
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-223-
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-224-  /**
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-225-   * Create a proxy object that will speak the binary protocol on a socket.
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-226-   * Upward messages are passed on the specified handler and downward
java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java-227-   * downward messages are public methods on this object.
--
java/org/apache/hadoop/mapred/TextOutputFormat.java:87:    public synchronized void write(K key, V value)
java/org/apache/hadoop/mapred/TextOutputFormat.java-88-      throws IOException {
java/org/apache/hadoop/mapred/TextOutputFormat.java-89-
java/org/apache/hadoop/mapred/TextOutputFormat.java-90-      boolean nullKey = key == null || key instanceof NullWritable;
java/org/apache/hadoop/mapred/TextOutputFormat.java-91-      boolean nullValue = value == null || value instanceof NullWritable;
java/org/apache/hadoop/mapred/TextOutputFormat.java-92-      if (nullKey && nullValue) {
java/org/apache/hadoop/mapred/TextOutputFormat.java-93-        return;
java/org/apache/hadoop/mapred/TextOutputFormat.java-94-      }
java/org/apache/hadoop/mapred/TextOutputFormat.java-95-      if (!nullKey) {
java/org/apache/hadoop/mapred/TextOutputFormat.java-96-        writeObject(key);
java/org/apache/hadoop/mapred/TextOutputFormat.java-97-      }
java/org/apache/hadoop/mapred/TextOutputFormat.java-98-      if (!(nullKey || nullValue)) {
java/org/apache/hadoop/mapred/TextOutputFormat.java-99-        out.write(keyValueSeparator);
java/org/apache/hadoop/mapred/TextOutputFormat.java-100-      }
java/org/apache/hadoop/mapred/TextOutputFormat.java-101-      if (!nullValue) {
java/org/apache/hadoop/mapred/TextOutputFormat.java-102-        writeObject(value);
java/org/apache/hadoop/mapred/TextOutputFormat.java-103-      }
java/org/apache/hadoop/mapred/TextOutputFormat.java-104-      out.write(newline);
java/org/apache/hadoop/mapred/TextOutputFormat.java-105-    }
java/org/apache/hadoop/mapred/TextOutputFormat.java-106-
java/org/apache/hadoop/mapred/TextOutputFormat.java-107-    public synchronized void close(Reporter reporter) throws IOException {
--
java/org/apache/hadoop/mapred/MapTask.java:122:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/MapTask.java-123-    super.write(out);
java/org/apache/hadoop/mapred/MapTask.java-124-    if (isMapOrReduce()) {
java/org/apache/hadoop/mapred/MapTask.java-125-      splitMetaInfo.write(out);
java/org/apache/hadoop/mapred/MapTask.java-126-      splitMetaInfo = null;
java/org/apache/hadoop/mapred/MapTask.java-127-    }
java/org/apache/hadoop/mapred/MapTask.java-128-  }
java/org/apache/hadoop/mapred/MapTask.java-129-  
java/org/apache/hadoop/mapred/MapTask.java-130-  @Override
java/org/apache/hadoop/mapred/MapTask.java-131-  public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapred/MapTask.java-132-    super.readFields(in);
java/org/apache/hadoop/mapred/MapTask.java-133-    if (isMapOrReduce()) {
java/org/apache/hadoop/mapred/MapTask.java-134-      splitMetaInfo.readFields(in);
java/org/apache/hadoop/mapred/MapTask.java-135-    }
java/org/apache/hadoop/mapred/MapTask.java-136-  }
java/org/apache/hadoop/mapred/MapTask.java-137-
java/org/apache/hadoop/mapred/MapTask.java-138-  /**
java/org/apache/hadoop/mapred/MapTask.java-139-   * This class wraps the user's record reader to update the counters and progress
java/org/apache/hadoop/mapred/MapTask.java-140-   * as records are read.
java/org/apache/hadoop/mapred/MapTask.java-141-   * @param <K>
java/org/apache/hadoop/mapred/MapTask.java-142-   * @param <V>
--
java/org/apache/hadoop/mapred/MapTask.java:631:    public void write(K key, V value) 
java/org/apache/hadoop/mapred/MapTask.java-632-    throws IOException, InterruptedException {
java/org/apache/hadoop/mapred/MapTask.java-633-      reporter.progress();
java/org/apache/hadoop/mapred/MapTask.java-634-      long bytesOutPrev = getOutputBytes(fsStats);
java/org/apache/hadoop/mapred/MapTask.java-635-      out.write(key, value);
java/org/apache/hadoop/mapred/MapTask.java-636-      long bytesOutCurr = getOutputBytes(fsStats);
java/org/apache/hadoop/mapred/MapTask.java-637-      fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
java/org/apache/hadoop/mapred/MapTask.java-638-      mapOutputRecordCounter.increment(1);
java/org/apache/hadoop/mapred/MapTask.java-639-    }
java/org/apache/hadoop/mapred/MapTask.java-640-
java/org/apache/hadoop/mapred/MapTask.java-641-    @Override
java/org/apache/hadoop/mapred/MapTask.java-642-    public void close(TaskAttemptContext context) 
java/org/apache/hadoop/mapred/MapTask.java-643-    throws IOException,InterruptedException {
java/org/apache/hadoop/mapred/MapTask.java-644-      reporter.progress();
java/org/apache/hadoop/mapred/MapTask.java-645-      if (out != null) {
java/org/apache/hadoop/mapred/MapTask.java-646-        long bytesOutPrev = getOutputBytes(fsStats);
java/org/apache/hadoop/mapred/MapTask.java-647-        out.close(context);
java/org/apache/hadoop/mapred/MapTask.java-648-        long bytesOutCurr = getOutputBytes(fsStats);
java/org/apache/hadoop/mapred/MapTask.java-649-        fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
java/org/apache/hadoop/mapred/MapTask.java-650-      }
java/org/apache/hadoop/mapred/MapTask.java-651-    }
--
java/org/apache/hadoop/mapred/MapTask.java:691:    public void write(K key, V value) throws IOException, InterruptedException {
java/org/apache/hadoop/mapred/MapTask.java-692-      collector.collect(key, value,
java/org/apache/hadoop/mapred/MapTask.java-693-                        partitioner.getPartition(key, value, partitions));
java/org/apache/hadoop/mapred/MapTask.java-694-    }
java/org/apache/hadoop/mapred/MapTask.java-695-
java/org/apache/hadoop/mapred/MapTask.java-696-    @Override
java/org/apache/hadoop/mapred/MapTask.java-697-    public void close(TaskAttemptContext context
java/org/apache/hadoop/mapred/MapTask.java-698-                      ) throws IOException,InterruptedException {
java/org/apache/hadoop/mapred/MapTask.java-699-      try {
java/org/apache/hadoop/mapred/MapTask.java-700-        collector.flush();
java/org/apache/hadoop/mapred/MapTask.java-701-      } catch (ClassNotFoundException cnf) {
java/org/apache/hadoop/mapred/MapTask.java-702-        throw new IOException("can't find class ", cnf);
java/org/apache/hadoop/mapred/MapTask.java-703-      }
java/org/apache/hadoop/mapred/MapTask.java-704-      collector.close();
java/org/apache/hadoop/mapred/MapTask.java-705-    }
java/org/apache/hadoop/mapred/MapTask.java-706-  }
java/org/apache/hadoop/mapred/MapTask.java-707-
java/org/apache/hadoop/mapred/MapTask.java-708-  @SuppressWarnings("unchecked")
java/org/apache/hadoop/mapred/MapTask.java-709-  private <INKEY,INVALUE,OUTKEY,OUTVALUE>
java/org/apache/hadoop/mapred/MapTask.java-710-  void runNewMapper(final JobConf job,
java/org/apache/hadoop/mapred/MapTask.java-711-                    final TaskSplitIndex splitIndex,
--
java/org/apache/hadoop/mapred/MapTask.java:1323:      public void write(int v)
java/org/apache/hadoop/mapred/MapTask.java-1324-          throws IOException {
java/org/apache/hadoop/mapred/MapTask.java-1325-        scratch[0] = (byte)v;
java/org/apache/hadoop/mapred/MapTask.java-1326-        write(scratch, 0, 1);
java/org/apache/hadoop/mapred/MapTask.java-1327-      }
java/org/apache/hadoop/mapred/MapTask.java-1328-
java/org/apache/hadoop/mapred/MapTask.java-1329-      /**
java/org/apache/hadoop/mapred/MapTask.java-1330-       * Attempt to write a sequence of bytes to the collection buffer.
java/org/apache/hadoop/mapred/MapTask.java-1331-       * This method will block if the spill thread is running and it
java/org/apache/hadoop/mapred/MapTask.java-1332-       * cannot write.
java/org/apache/hadoop/mapred/MapTask.java-1333-       * @throws MapBufferTooSmallException if record is too large to
java/org/apache/hadoop/mapred/MapTask.java-1334-       *    deserialize into the collection buffer.
java/org/apache/hadoop/mapred/MapTask.java-1335-       */
java/org/apache/hadoop/mapred/MapTask.java-1336-      @Override
java/org/apache/hadoop/mapred/MapTask.java:1337:      public void write(byte b[], int off, int len)
java/org/apache/hadoop/mapred/MapTask.java-1338-          throws IOException {
java/org/apache/hadoop/mapred/MapTask.java-1339-        // must always verify the invariant that at least METASIZE bytes are
java/org/apache/hadoop/mapred/MapTask.java-1340-        // available beyond kvindex, even when len == 0
java/org/apache/hadoop/mapred/MapTask.java-1341-        bufferRemaining -= len;
java/org/apache/hadoop/mapred/MapTask.java-1342-        if (bufferRemaining <= 0) {
java/org/apache/hadoop/mapred/MapTask.java-1343-          // writing these bytes could exhaust available buffer space or fill
java/org/apache/hadoop/mapred/MapTask.java-1344-          // the buffer to soft limit. check if spill or blocking are necessary
java/org/apache/hadoop/mapred/MapTask.java-1345-          boolean blockwrite = false;
java/org/apache/hadoop/mapred/MapTask.java-1346-          spillLock.lock();
java/org/apache/hadoop/mapred/MapTask.java-1347-          try {
java/org/apache/hadoop/mapred/MapTask.java-1348-            do {
java/org/apache/hadoop/mapred/MapTask.java-1349-              checkSpillException();
java/org/apache/hadoop/mapred/MapTask.java-1350-
java/org/apache/hadoop/mapred/MapTask.java-1351-              final int kvbidx = 4 * kvindex;
java/org/apache/hadoop/mapred/MapTask.java-1352-              final int kvbend = 4 * kvend;
java/org/apache/hadoop/mapred/MapTask.java-1353-              // ser distance to key index
java/org/apache/hadoop/mapred/MapTask.java-1354-              final int distkvi = distanceTo(bufindex, kvbidx);
java/org/apache/hadoop/mapred/MapTask.java-1355-              // ser distance to spill end index
java/org/apache/hadoop/mapred/MapTask.java-1356-              final int distkve = distanceTo(bufindex, kvbend);
java/org/apache/hadoop/mapred/MapTask.java-1357-
--
java/org/apache/hadoop/mapred/RecordWriter.java:46:  void write(K key, V value) throws IOException;
java/org/apache/hadoop/mapred/RecordWriter.java-47-
java/org/apache/hadoop/mapred/RecordWriter.java-48-  /** 
java/org/apache/hadoop/mapred/RecordWriter.java-49-   * Close this <code>RecordWriter</code> to future operations.
java/org/apache/hadoop/mapred/RecordWriter.java-50-   * 
java/org/apache/hadoop/mapred/RecordWriter.java-51-   * @param reporter facility to report progress.
java/org/apache/hadoop/mapred/RecordWriter.java-52-   * @throws IOException
java/org/apache/hadoop/mapred/RecordWriter.java-53-   */ 
java/org/apache/hadoop/mapred/RecordWriter.java-54-  void close(Reporter reporter) throws IOException;
java/org/apache/hadoop/mapred/RecordWriter.java-55-}
--
java/org/apache/hadoop/mapred/ReduceTask.java:180:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/ReduceTask.java-181-    super.write(out);
java/org/apache/hadoop/mapred/ReduceTask.java-182-
java/org/apache/hadoop/mapred/ReduceTask.java-183-    out.writeInt(numMaps);                        // write the number of maps
java/org/apache/hadoop/mapred/ReduceTask.java-184-  }
java/org/apache/hadoop/mapred/ReduceTask.java-185-
java/org/apache/hadoop/mapred/ReduceTask.java-186-  @Override
java/org/apache/hadoop/mapred/ReduceTask.java-187-  public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapred/ReduceTask.java-188-    super.readFields(in);
java/org/apache/hadoop/mapred/ReduceTask.java-189-
java/org/apache/hadoop/mapred/ReduceTask.java-190-    numMaps = in.readInt();
java/org/apache/hadoop/mapred/ReduceTask.java-191-  }
java/org/apache/hadoop/mapred/ReduceTask.java-192-  
java/org/apache/hadoop/mapred/ReduceTask.java-193-  // Get the input files for the reducer (for local jobs).
java/org/apache/hadoop/mapred/ReduceTask.java-194-  private Path[] getMapFiles(FileSystem fs) throws IOException {
java/org/apache/hadoop/mapred/ReduceTask.java-195-    List<Path> fileList = new ArrayList<Path>();
java/org/apache/hadoop/mapred/ReduceTask.java-196-    for(int i = 0; i < numMaps; ++i) {
java/org/apache/hadoop/mapred/ReduceTask.java-197-      fileList.add(mapOutputFile.getInputFile(i));
java/org/apache/hadoop/mapred/ReduceTask.java-198-    }
java/org/apache/hadoop/mapred/ReduceTask.java-199-    return fileList.toArray(new Path[0]);
java/org/apache/hadoop/mapred/ReduceTask.java-200-  }
--
java/org/apache/hadoop/mapred/ReduceTask.java:491:    public void write(K key, V value) throws IOException {
java/org/apache/hadoop/mapred/ReduceTask.java-492-      long bytesOutPrev = getOutputBytes(fsStats);
java/org/apache/hadoop/mapred/ReduceTask.java-493-      real.write(key, value);
java/org/apache/hadoop/mapred/ReduceTask.java-494-      long bytesOutCurr = getOutputBytes(fsStats);
java/org/apache/hadoop/mapred/ReduceTask.java-495-      fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
java/org/apache/hadoop/mapred/ReduceTask.java-496-      reduceOutputCounter.increment(1);
java/org/apache/hadoop/mapred/ReduceTask.java-497-    }
java/org/apache/hadoop/mapred/ReduceTask.java-498-
java/org/apache/hadoop/mapred/ReduceTask.java-499-    @Override
java/org/apache/hadoop/mapred/ReduceTask.java-500-    public void close(Reporter reporter) throws IOException {
java/org/apache/hadoop/mapred/ReduceTask.java-501-      long bytesOutPrev = getOutputBytes(fsStats);
java/org/apache/hadoop/mapred/ReduceTask.java-502-      real.close(reporter);
java/org/apache/hadoop/mapred/ReduceTask.java-503-      long bytesOutCurr = getOutputBytes(fsStats);
java/org/apache/hadoop/mapred/ReduceTask.java-504-      fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
java/org/apache/hadoop/mapred/ReduceTask.java-505-    }
java/org/apache/hadoop/mapred/ReduceTask.java-506-
java/org/apache/hadoop/mapred/ReduceTask.java-507-    private long getOutputBytes(List<Statistics> stats) {
java/org/apache/hadoop/mapred/ReduceTask.java-508-      if (stats == null) return 0;
java/org/apache/hadoop/mapred/ReduceTask.java-509-      long bytesWritten = 0;
java/org/apache/hadoop/mapred/ReduceTask.java-510-      for (Statistics stat: stats) {
java/org/apache/hadoop/mapred/ReduceTask.java-511-        bytesWritten = bytesWritten + stat.getBytesWritten();
--
java/org/apache/hadoop/mapred/ReduceTask.java:556:    public void write(K key, V value) throws IOException, InterruptedException {
java/org/apache/hadoop/mapred/ReduceTask.java-557-      long bytesOutPrev = getOutputBytes(fsStats);
java/org/apache/hadoop/mapred/ReduceTask.java-558-      real.write(key,value);
java/org/apache/hadoop/mapred/ReduceTask.java-559-      long bytesOutCurr = getOutputBytes(fsStats);
java/org/apache/hadoop/mapred/ReduceTask.java-560-      fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
java/org/apache/hadoop/mapred/ReduceTask.java-561-      outputRecordCounter.increment(1);
java/org/apache/hadoop/mapred/ReduceTask.java-562-    }
java/org/apache/hadoop/mapred/ReduceTask.java-563-
java/org/apache/hadoop/mapred/ReduceTask.java-564-    private long getOutputBytes(List<Statistics> stats) {
java/org/apache/hadoop/mapred/ReduceTask.java-565-      if (stats == null) return 0;
java/org/apache/hadoop/mapred/ReduceTask.java-566-      long bytesWritten = 0;
java/org/apache/hadoop/mapred/ReduceTask.java-567-      for (Statistics stat: stats) {
java/org/apache/hadoop/mapred/ReduceTask.java-568-        bytesWritten = bytesWritten + stat.getBytesWritten();
java/org/apache/hadoop/mapred/ReduceTask.java-569-      }
java/org/apache/hadoop/mapred/ReduceTask.java-570-      return bytesWritten;
java/org/apache/hadoop/mapred/ReduceTask.java-571-    }
java/org/apache/hadoop/mapred/ReduceTask.java-572-  }
java/org/apache/hadoop/mapred/ReduceTask.java-573-
java/org/apache/hadoop/mapred/ReduceTask.java-574-  @SuppressWarnings("unchecked")
java/org/apache/hadoop/mapred/ReduceTask.java-575-  private <INKEY,INVALUE,OUTKEY,OUTVALUE>
java/org/apache/hadoop/mapred/ReduceTask.java-576-  void runNewReducer(JobConf job,
--
java/org/apache/hadoop/mapred/ClusterStatus.java:150:    public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/ClusterStatus.java-151-      Text.writeString(out, trackerName);
java/org/apache/hadoop/mapred/ClusterStatus.java-152-      Text.writeString(out, reasonForBlackListing);
java/org/apache/hadoop/mapred/ClusterStatus.java-153-      Text.writeString(out, blackListReport);
java/org/apache/hadoop/mapred/ClusterStatus.java-154-    }
java/org/apache/hadoop/mapred/ClusterStatus.java-155-
java/org/apache/hadoop/mapred/ClusterStatus.java-156-    @Override
java/org/apache/hadoop/mapred/ClusterStatus.java-157-    /**
java/org/apache/hadoop/mapred/ClusterStatus.java-158-     * Print information related to the blacklisted tasktracker in a
java/org/apache/hadoop/mapred/ClusterStatus.java-159-     * whitespace separated fashion.
java/org/apache/hadoop/mapred/ClusterStatus.java-160-     * 
java/org/apache/hadoop/mapred/ClusterStatus.java-161-     * The method changes any newlines in the report describing why
java/org/apache/hadoop/mapred/ClusterStatus.java-162-     * the tasktracker was blacklisted to a ':' for enabling better
java/org/apache/hadoop/mapred/ClusterStatus.java-163-     * parsing.
java/org/apache/hadoop/mapred/ClusterStatus.java-164-     */
java/org/apache/hadoop/mapred/ClusterStatus.java-165-    public String toString() {
java/org/apache/hadoop/mapred/ClusterStatus.java-166-      StringBuilder sb = new StringBuilder();
java/org/apache/hadoop/mapred/ClusterStatus.java-167-      sb.append(trackerName);
java/org/apache/hadoop/mapred/ClusterStatus.java-168-      sb.append("\t");
java/org/apache/hadoop/mapred/ClusterStatus.java-169-      sb.append(reasonForBlackListing);
java/org/apache/hadoop/mapred/ClusterStatus.java-170-      sb.append("\t");
--
java/org/apache/hadoop/mapred/ClusterStatus.java:478:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/ClusterStatus.java-479-    if (activeTrackers.size() == 0) {
java/org/apache/hadoop/mapred/ClusterStatus.java-480-      out.writeInt(numActiveTrackers);
java/org/apache/hadoop/mapred/ClusterStatus.java-481-      out.writeInt(0);
java/org/apache/hadoop/mapred/ClusterStatus.java-482-    } else {
java/org/apache/hadoop/mapred/ClusterStatus.java-483-      out.writeInt(activeTrackers.size());
java/org/apache/hadoop/mapred/ClusterStatus.java-484-      out.writeInt(activeTrackers.size());
java/org/apache/hadoop/mapred/ClusterStatus.java-485-      for (String tracker : activeTrackers) {
java/org/apache/hadoop/mapred/ClusterStatus.java-486-        Text.writeString(out, tracker);
java/org/apache/hadoop/mapred/ClusterStatus.java-487-      }
java/org/apache/hadoop/mapred/ClusterStatus.java-488-    }
java/org/apache/hadoop/mapred/ClusterStatus.java-489-    if (blacklistedTrackersInfo.size() == 0) {
java/org/apache/hadoop/mapred/ClusterStatus.java-490-      out.writeInt(numBlacklistedTrackers);
java/org/apache/hadoop/mapred/ClusterStatus.java-491-      out.writeInt(blacklistedTrackersInfo.size());
java/org/apache/hadoop/mapred/ClusterStatus.java-492-    } else {
java/org/apache/hadoop/mapred/ClusterStatus.java-493-      out.writeInt(blacklistedTrackersInfo.size());
java/org/apache/hadoop/mapred/ClusterStatus.java-494-      out.writeInt(blacklistedTrackersInfo.size());
java/org/apache/hadoop/mapred/ClusterStatus.java-495-      for (BlackListInfo tracker : blacklistedTrackersInfo) {
java/org/apache/hadoop/mapred/ClusterStatus.java-496-        tracker.write(out);
java/org/apache/hadoop/mapred/ClusterStatus.java-497-      }
java/org/apache/hadoop/mapred/ClusterStatus.java-498-    }
--
java/org/apache/hadoop/mapred/Task.java:467:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/Task.java-468-    Text.writeString(out, jobFile);
java/org/apache/hadoop/mapred/Task.java-469-    taskId.write(out);
java/org/apache/hadoop/mapred/Task.java-470-    out.writeInt(partition);
java/org/apache/hadoop/mapred/Task.java-471-    out.writeInt(numSlotsRequired);
java/org/apache/hadoop/mapred/Task.java-472-    taskStatus.write(out);
java/org/apache/hadoop/mapred/Task.java-473-    skipRanges.write(out);
java/org/apache/hadoop/mapred/Task.java-474-    out.writeBoolean(skipping);
java/org/apache/hadoop/mapred/Task.java-475-    out.writeBoolean(jobCleanup);
java/org/apache/hadoop/mapred/Task.java-476-    if (jobCleanup) {
java/org/apache/hadoop/mapred/Task.java-477-      WritableUtils.writeEnum(out, jobRunStateForCleanup);
java/org/apache/hadoop/mapred/Task.java-478-    }
java/org/apache/hadoop/mapred/Task.java-479-    out.writeBoolean(jobSetup);
java/org/apache/hadoop/mapred/Task.java-480-    out.writeBoolean(writeSkipRecs);
java/org/apache/hadoop/mapred/Task.java-481-    out.writeBoolean(taskCleanup);
java/org/apache/hadoop/mapred/Task.java-482-    Text.writeString(out, user);
java/org/apache/hadoop/mapred/Task.java-483-    extraData.write(out);
java/org/apache/hadoop/mapred/Task.java-484-  }
java/org/apache/hadoop/mapred/Task.java-485-  
java/org/apache/hadoop/mapred/Task.java-486-  public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapred/Task.java-487-    jobFile = StringInterner.weakIntern(Text.readString(in));
--
java/org/apache/hadoop/mapred/Task.java:1644:      public void write(K key, V value
java/org/apache/hadoop/mapred/Task.java-1645-                        ) throws IOException, InterruptedException {
java/org/apache/hadoop/mapred/Task.java-1646-        output.collect(key,value);
java/org/apache/hadoop/mapred/Task.java-1647-      }
java/org/apache/hadoop/mapred/Task.java-1648-    }
java/org/apache/hadoop/mapred/Task.java-1649-
java/org/apache/hadoop/mapred/Task.java-1650-    @SuppressWarnings("unchecked")
java/org/apache/hadoop/mapred/Task.java-1651-    @Override
java/org/apache/hadoop/mapred/Task.java-1652-    public void combine(RawKeyValueIterator iterator, 
java/org/apache/hadoop/mapred/Task.java-1653-                 OutputCollector<K,V> collector
java/org/apache/hadoop/mapred/Task.java-1654-                 ) throws IOException, InterruptedException,
java/org/apache/hadoop/mapred/Task.java-1655-                          ClassNotFoundException {
java/org/apache/hadoop/mapred/Task.java-1656-      // make a reducer
java/org/apache/hadoop/mapred/Task.java-1657-      org.apache.hadoop.mapreduce.Reducer<K,V,K,V> reducer =
java/org/apache/hadoop/mapred/Task.java-1658-        (org.apache.hadoop.mapreduce.Reducer<K,V,K,V>)
java/org/apache/hadoop/mapred/Task.java-1659-          ReflectionUtils.newInstance(reducerClass, job);
java/org/apache/hadoop/mapred/Task.java-1660-      org.apache.hadoop.mapreduce.Reducer.Context 
java/org/apache/hadoop/mapred/Task.java-1661-           reducerContext = createReduceContext(reducer, job, taskId,
java/org/apache/hadoop/mapred/Task.java-1662-                                                iterator, null, inputCounter, 
java/org/apache/hadoop/mapred/Task.java-1663-                                                new OutputConverter(collector),
java/org/apache/hadoop/mapred/Task.java-1664-                                                committer,
--
java/org/apache/hadoop/mapred/BackupStore.java:124:  public void write(DataInputBuffer key, DataInputBuffer value)
java/org/apache/hadoop/mapred/BackupStore.java-125-  throws IOException {
java/org/apache/hadoop/mapred/BackupStore.java-126-
java/org/apache/hadoop/mapred/BackupStore.java-127-    assert (key != null && value != null);
java/org/apache/hadoop/mapred/BackupStore.java-128-
java/org/apache/hadoop/mapred/BackupStore.java-129-    if (fileCache.isActive()) {
java/org/apache/hadoop/mapred/BackupStore.java-130-      fileCache.write(key, value);
java/org/apache/hadoop/mapred/BackupStore.java-131-      return;
java/org/apache/hadoop/mapred/BackupStore.java-132-    }
java/org/apache/hadoop/mapred/BackupStore.java-133-
java/org/apache/hadoop/mapred/BackupStore.java-134-    if (memCache.reserveSpace(key, value)) {
java/org/apache/hadoop/mapred/BackupStore.java-135-      memCache.write(key, value);
java/org/apache/hadoop/mapred/BackupStore.java-136-    } else {
java/org/apache/hadoop/mapred/BackupStore.java-137-      fileCache.activate();
java/org/apache/hadoop/mapred/BackupStore.java-138-      fileCache.write(key, value);
java/org/apache/hadoop/mapred/BackupStore.java-139-    }
java/org/apache/hadoop/mapred/BackupStore.java-140-  }
java/org/apache/hadoop/mapred/BackupStore.java-141-
java/org/apache/hadoop/mapred/BackupStore.java-142-  public void mark() throws IOException {
java/org/apache/hadoop/mapred/BackupStore.java-143-
java/org/apache/hadoop/mapred/BackupStore.java-144-    // We read one KV pair in advance in hasNext. 
--
java/org/apache/hadoop/mapred/BackupStore.java:466:    public void write(DataInputBuffer key, DataInputBuffer value)
java/org/apache/hadoop/mapred/BackupStore.java-467-    throws IOException {
java/org/apache/hadoop/mapred/BackupStore.java-468-      int keyLength = key.getLength() - key.getPosition();
java/org/apache/hadoop/mapred/BackupStore.java-469-      int valueLength = value.getLength() - value.getPosition();
java/org/apache/hadoop/mapred/BackupStore.java-470-      WritableUtils.writeVInt(dataOut, keyLength);
java/org/apache/hadoop/mapred/BackupStore.java-471-      WritableUtils.writeVInt(dataOut, valueLength);
java/org/apache/hadoop/mapred/BackupStore.java-472-      dataOut.write(key.getData(), key.getPosition(), keyLength);
java/org/apache/hadoop/mapred/BackupStore.java-473-      dataOut.write(value.getData(), value.getPosition(), valueLength);
java/org/apache/hadoop/mapred/BackupStore.java-474-      usedSize += keyLength + valueLength + 
java/org/apache/hadoop/mapred/BackupStore.java-475-        WritableUtils.getVIntSize(keyLength) +
java/org/apache/hadoop/mapred/BackupStore.java-476-        WritableUtils.getVIntSize(valueLength);
java/org/apache/hadoop/mapred/BackupStore.java-477-      LOG.debug("ID: " + segmentList.size() + " WRITE TO MEM");
java/org/apache/hadoop/mapred/BackupStore.java-478-    }
java/org/apache/hadoop/mapred/BackupStore.java-479-
java/org/apache/hadoop/mapred/BackupStore.java-480-    /**
java/org/apache/hadoop/mapred/BackupStore.java-481-     * This method creates a memory segment from the existing buffer
java/org/apache/hadoop/mapred/BackupStore.java-482-     * @throws IOException
java/org/apache/hadoop/mapred/BackupStore.java-483-     */
java/org/apache/hadoop/mapred/BackupStore.java-484-    void createInMemorySegment () throws IOException {
java/org/apache/hadoop/mapred/BackupStore.java-485-
java/org/apache/hadoop/mapred/BackupStore.java-486-      // If nothing was written in this block because the record size
--
java/org/apache/hadoop/mapred/BackupStore.java:532:    void write(DataInputBuffer key, DataInputBuffer value)
java/org/apache/hadoop/mapred/BackupStore.java-533-    throws IOException {
java/org/apache/hadoop/mapred/BackupStore.java-534-      if (writer == null) {
java/org/apache/hadoop/mapred/BackupStore.java-535-        // If spillNumber is 0, we should have called activate and not
java/org/apache/hadoop/mapred/BackupStore.java-536-        // come here at all
java/org/apache/hadoop/mapred/BackupStore.java-537-        assert (spillNumber != 0); 
java/org/apache/hadoop/mapred/BackupStore.java-538-        writer = createSpillFile();
java/org/apache/hadoop/mapred/BackupStore.java-539-      }
java/org/apache/hadoop/mapred/BackupStore.java-540-      writer.append(key, value);
java/org/apache/hadoop/mapred/BackupStore.java-541-      LOG.debug("ID: " + segmentList.size() + " WRITE TO DISK");
java/org/apache/hadoop/mapred/BackupStore.java-542-    }
java/org/apache/hadoop/mapred/BackupStore.java-543-
java/org/apache/hadoop/mapred/BackupStore.java-544-    void reinitialize() {
java/org/apache/hadoop/mapred/BackupStore.java-545-      spillNumber = 0;
java/org/apache/hadoop/mapred/BackupStore.java-546-      writer = null;
java/org/apache/hadoop/mapred/BackupStore.java-547-      isActive = false;
java/org/apache/hadoop/mapred/BackupStore.java-548-    }
java/org/apache/hadoop/mapred/BackupStore.java-549-
java/org/apache/hadoop/mapred/BackupStore.java-550-    void activate() throws IOException {
java/org/apache/hadoop/mapred/BackupStore.java-551-      isActive = true;
java/org/apache/hadoop/mapred/BackupStore.java-552-      writer = createSpillFile();
--
java/org/apache/hadoop/mapred/JobProfile.java:169:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/JobProfile.java-170-    jobid.write(out);
java/org/apache/hadoop/mapred/JobProfile.java-171-    Text.writeString(out, jobFile);
java/org/apache/hadoop/mapred/JobProfile.java-172-    Text.writeString(out, url);
java/org/apache/hadoop/mapred/JobProfile.java-173-    Text.writeString(out, user);
java/org/apache/hadoop/mapred/JobProfile.java-174-    Text.writeString(out, name);
java/org/apache/hadoop/mapred/JobProfile.java-175-    Text.writeString(out, queueName);
java/org/apache/hadoop/mapred/JobProfile.java-176-  }
java/org/apache/hadoop/mapred/JobProfile.java-177-
java/org/apache/hadoop/mapred/JobProfile.java-178-  public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapred/JobProfile.java-179-    jobid.readFields(in);
java/org/apache/hadoop/mapred/JobProfile.java-180-    this.jobFile = StringInterner.weakIntern(Text.readString(in));
java/org/apache/hadoop/mapred/JobProfile.java-181-    this.url = StringInterner.weakIntern(Text.readString(in));
java/org/apache/hadoop/mapred/JobProfile.java-182-    this.user = StringInterner.weakIntern(Text.readString(in));
java/org/apache/hadoop/mapred/JobProfile.java-183-    this.name = StringInterner.weakIntern(Text.readString(in));
java/org/apache/hadoop/mapred/JobProfile.java-184-    this.queueName = StringInterner.weakIntern(Text.readString(in));
java/org/apache/hadoop/mapred/JobProfile.java-185-  }
java/org/apache/hadoop/mapred/JobProfile.java-186-}
java/org/apache/hadoop/mapred/JobProfile.java-187-
java/org/apache/hadoop/mapred/JobProfile.java-188-
--
java/org/apache/hadoop/mapred/JvmContext.java:53:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapred/JvmContext.java-54-    jvmId.write(out);
java/org/apache/hadoop/mapred/JvmContext.java-55-    Text.writeString(out, pid);
java/org/apache/hadoop/mapred/JvmContext.java-56-  }
java/org/apache/hadoop/mapred/JvmContext.java-57-}
--
java/org/apache/hadoop/mapred/MapFileOutputFormat.java:74:        public void write(WritableComparable key, Writable value)
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-75-          throws IOException {
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-76-
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-77-          out.append(key, value);
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-78-        }
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-79-
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-80-        public void close(Reporter reporter) throws IOException { out.close();}
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-81-      };
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-82-  }
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-83-
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-84-  /** Open the output generated by this format. */
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-85-  public static MapFile.Reader[] getReaders(FileSystem ignored, Path dir,
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-86-                                            Configuration conf)
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-87-      throws IOException {
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-88-    return org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-89-      getReaders(dir, conf);
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-90-  }
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-91-    
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-92-  /** Get an entry from output generated by this class. */
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-93-  public static <K extends WritableComparable, V extends Writable>
java/org/apache/hadoop/mapred/MapFileOutputFormat.java-94-  Writable getEntry(MapFile.Reader[] readers,
--
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java:110:    public void write(KEYOUT key, VALUEOUT value) throws IOException,
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-111-        InterruptedException {
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-112-      mapContext.write(key, value);
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-113-    }
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-114-
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-115-    @Override
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-116-    public String getStatus() {
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-117-      return mapContext.getStatus();
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-118-    }
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-119-
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-120-    @Override
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-121-    public TaskAttemptID getTaskAttemptID() {
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-122-      return mapContext.getTaskAttemptID();
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-123-    }
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-124-
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-125-    @Override
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-126-    public void setStatus(String msg) {
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-127-      mapContext.setStatus(msg);
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-128-    }
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-129-
java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java-130-    @Override
--
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java:213:    public void write(K2 key, V2 value) throws IOException,
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-214-                                               InterruptedException {
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-215-      synchronized (outer) {
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-216-        outer.write(key, value);
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-217-      }
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-218-    }  
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-219-  }
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-220-
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-221-  private class SubMapStatusReporter extends StatusReporter {
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-222-
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-223-    @Override
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-224-    public Counter getCounter(Enum<?> name) {
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-225-      return outer.getCounter(name);
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-226-    }
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-227-
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-228-    @Override
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-229-    public Counter getCounter(String group, String name) {
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-230-      return outer.getCounter(group, name);
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-231-    }
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-232-
java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java-233-    @Override
--
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java:160:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-161-    out.writeLong(totLength);
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-162-    out.writeInt(lengths.length);
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-163-    for(long length : lengths) {
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-164-      out.writeLong(length);
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-165-    }
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-166-    out.writeInt(paths.length);
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-167-    for(Path p : paths) {
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-168-      Text.writeString(out, p.toString());
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-169-    }
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-170-    out.writeInt(startoffset.length);
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-171-    for(long length : startoffset) {
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-172-      out.writeLong(length);
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-173-    }
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-174-  }
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-175-  
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-176-  @Override
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-177- public String toString() {
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-178-    StringBuffer sb = new StringBuffer();
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-179-    for (int i = 0; i < paths.length; i++) {
java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java-180-      if (i == 0 ) {
--
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java:79:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-80-    Text.writeString(out, file.toString());
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-81-    out.writeLong(start);
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-82-    out.writeLong(length);
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-83-  }
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-84-
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-85-  @Override
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-86-  public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-87-    file = new Path(Text.readString(in));
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-88-    start = in.readLong();
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-89-    length = in.readLong();
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-90-    hosts = null;
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-91-  }
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-92-
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-93-  @Override
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-94-  public String[] getLocations() throws IOException {
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-95-    if (this.hosts == null) {
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-96-      return new String[]{};
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-97-    } else {
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-98-      return this.hosts;
java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java-99-    }
--
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java:141:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-142-    Text.writeString(out, inputSplitClass.getName());
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-143-    Text.writeString(out, inputFormatClass.getName());
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-144-    Text.writeString(out, mapperClass.getName());
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-145-    SerializationFactory factory = new SerializationFactory(conf);
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-146-    Serializer serializer = 
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-147-          factory.getSerializer(inputSplitClass);
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-148-    serializer.open((DataOutputStream)out);
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-149-    serializer.serialize(inputSplit);
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-150-  }
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-151-
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-152-  public Configuration getConf() {
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-153-    return conf;
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-154-  }
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-155-
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-156-  public void setConf(Configuration conf) {
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-157-    this.conf = conf;
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-158-  }
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-159-
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-160-  @Override
java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java-161-  public String toString() {
--
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java:124:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-125-    WritableUtils.writeVInt(out, splits.length);
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-126-    for (InputSplit s : splits) {
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-127-      Text.writeString(out, s.getClass().getName());
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-128-    }
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-129-    for (InputSplit s : splits) {
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-130-      SerializationFactory factory = new SerializationFactory(conf);
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-131-      Serializer serializer = 
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-132-        factory.getSerializer(s.getClass());
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-133-      serializer.open((DataOutputStream)out);
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-134-      serializer.serialize(s);
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-135-    }
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-136-  }
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-137-
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-138-  /**
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-139-   * {@inheritDoc}
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-140-   * @throws IOException If the child InputSplit cannot be read, typically
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-141-   *                     for failing access checks.
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-142-   */
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-143-  @SuppressWarnings("unchecked")  // Generic array assignment
java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java-144-  public void readFields(DataInput in) throws IOException {
--
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java:170:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-171-    WritableUtils.writeVInt(out, values.length);
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-172-    writeBitSet(out, values.length, written);
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-173-    for (int i = 0; i < values.length; ++i) {
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-174-      Text.writeString(out, values[i].getClass().getName());
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-175-    }
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-176-    for (int i = 0; i < values.length; ++i) {
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-177-      if (has(i)) {
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-178-        values[i].write(out);
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-179-      }
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-180-    }
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-181-  }
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-182-
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-183-  /**
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-184-   * {@inheritDoc}
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-185-   */
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-186-  @SuppressWarnings("unchecked") // No static typeinfo on Tuples
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-187-  public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-188-    int card = WritableUtils.readVInt(in);
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-189-    values = new Writable[card];
java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java-190-    readBitSet(in, card, written);
--
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java:111:    public void write(K key, V value) throws IOException, InterruptedException {
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java-112-      if (rawWriter == null) {
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java-113-        rawWriter = outputFormat.getRecordWriter(taskContext);
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java-114-      }
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java-115-      rawWriter.write(key, value);
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java-116-    }
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java-117-
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java-118-    @Override
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java-119-    public void close(TaskAttemptContext context) 
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java-120-    throws IOException, InterruptedException {
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java-121-      if (rawWriter != null) {
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java-122-        rawWriter.close(context);
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java-123-      }
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java-124-    }
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java-125-
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java-126-  }
java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java-127-}
--
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java:80:        public void write(K key, V value)
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-81-          throws IOException {
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-82-
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-83-          out.append(key, value);
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-84-        }
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-85-
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-86-        public void close(TaskAttemptContext context) throws IOException { 
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-87-          out.close();
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-88-        }
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-89-      };
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-90-  }
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-91-
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-92-  /**
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-93-   * Get the {@link CompressionType} for the output {@link SequenceFile}.
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-94-   * @param job the {@link Job}
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-95-   * @return the {@link CompressionType} for the output {@link SequenceFile}, 
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-96-   *         defaulting to {@link CompressionType#RECORD}
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-97-   */
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-98-  public static CompressionType getOutputCompressionType(JobContext job) {
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-99-    String val = job.getConfiguration().get(FileOutputFormat.COMPRESS_TYPE, 
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java-100-                                            CompressionType.RECORD.toString());
--
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java:94:    public void write(K key, V value) throws IOException, InterruptedException {
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-95-      getRawWriter().write(key, value);
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-96-    }
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-97-
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-98-    @Override
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-99-    public void close(TaskAttemptContext context) 
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-100-    throws IOException, InterruptedException {
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-101-      getRawWriter().close(context);
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-102-    }
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-103-    
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-104-    private RecordWriter<K,V> getRawWriter() throws IOException {
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-105-      if (rawWriter == null) {
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-106-        throw new IOException("Record Writer not set for FilterRecordWriter");
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-107-      }
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-108-      return rawWriter;
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-109-    }
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-110-  }
java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java-111-}
--
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java:143:      public void write(BytesWritable bkey, BytesWritable bvalue)
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-144-        throws IOException {
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-145-        wvaluebytes.reset(bvalue);
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-146-        out.appendRaw(bkey.getBytes(), 0, bkey.getLength(), wvaluebytes);
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-147-        wvaluebytes.reset(null);
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-148-      }
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-149-
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-150-      public void close(TaskAttemptContext context) throws IOException { 
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-151-        out.close();
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-152-      }
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-153-    };
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-154-  }
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-155-
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-156-  @Override 
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-157-  public void checkOutputSpecs(JobContext job) throws IOException {
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-158-    super.checkOutputSpecs(job);
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-159-    if (getCompressOutput(job) && 
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-160-        getOutputCompressionType(job) == CompressionType.RECORD ) {
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-161-      throw new InvalidJobConfException("SequenceFileAsBinaryOutputFormat "
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-162-        + "doesn't support Record Compression" );
java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java-163-    }
--
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java:42:        public void write(K key, V value) { }
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-43-        public void close(TaskAttemptContext context) { }
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-44-      };
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-45-  }
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-46-  
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-47-  @Override
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-48-  public void checkOutputSpecs(JobContext context) { }
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-49-  
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-50-  @Override
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-51-  public OutputCommitter getOutputCommitter(TaskAttemptContext context) {
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-52-    return new OutputCommitter() {
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-53-      public void abortTask(TaskAttemptContext taskContext) { }
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-54-      public void cleanupJob(JobContext jobContext) { }
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-55-      public void commitTask(TaskAttemptContext taskContext) { }
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-56-      public boolean needsTaskCommit(TaskAttemptContext taskContext) {
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-57-        return false;
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-58-      }
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-59-      public void setupJob(JobContext jobContext) { }
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-60-      public void setupTask(TaskAttemptContext taskContext) { }
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-61-
java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java-62-      @Override
--
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java:89:    public synchronized void write(K key, V value)
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-90-      throws IOException {
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-91-
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-92-      boolean nullKey = key == null || key instanceof NullWritable;
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-93-      boolean nullValue = value == null || value instanceof NullWritable;
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-94-      if (nullKey && nullValue) {
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-95-        return;
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-96-      }
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-97-      if (!nullKey) {
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-98-        writeObject(key);
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-99-      }
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-100-      if (!(nullKey || nullValue)) {
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-101-        out.write(keyValueSeparator);
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-102-      }
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-103-      if (!nullValue) {
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-104-        writeObject(value);
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-105-      }
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-106-      out.write(newline);
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-107-    }
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-108-
java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java-109-    public synchronized 
--
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java:364:    public void write(Object key, Object value) 
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-365-        throws IOException, InterruptedException {
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-366-      context.getCounter(COUNTERS_GROUP, counterName).increment(1);
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-367-      writer.write(key, value);
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-368-    }
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-369-
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-370-    public void close(TaskAttemptContext context) 
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-371-        throws IOException, InterruptedException {
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-372-      writer.close(context);
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-373-    }
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-374-  }
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-375-
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-376-  // instance code, to be used from Mapper/Reducer code
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-377-
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-378-  private TaskInputOutputContext<?, ?, KEYOUT, VALUEOUT> context;
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-379-  private Set<String> namedOutputs;
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-380-  private Map<String, RecordWriter<?, ?>> recordWriters;
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-381-  private boolean countersEnabled;
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-382-  
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-383-  /**
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-384-   * Creates and initializes multiple outputs support,
--
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java:409:  public <K, V> void write(String namedOutput, K key, V value)
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-410-      throws IOException, InterruptedException {
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-411-    write(namedOutput, key, value, namedOutput);
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-412-  }
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-413-
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-414-  /**
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-415-   * Write key and value to baseOutputPath using the namedOutput.
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-416-   * 
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-417-   * @param namedOutput    the named output name
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-418-   * @param key            the key
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-419-   * @param value          the value
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-420-   * @param baseOutputPath base-output path to write the record to.
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-421-   * Note: Framework will generate unique filename for the baseOutputPath
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-422-   */
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-423-  @SuppressWarnings("unchecked")
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java:424:  public <K, V> void write(String namedOutput, K key, V value,
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-425-      String baseOutputPath) throws IOException, InterruptedException {
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-426-    checkNamedOutputName(context, namedOutput, false);
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-427-    checkBaseOutputPath(baseOutputPath);
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-428-    if (!namedOutputs.contains(namedOutput)) {
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-429-      throw new IllegalArgumentException("Undefined named output '" +
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-430-        namedOutput + "'");
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-431-    }
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-432-    TaskAttemptContext taskContext = getContext(namedOutput);
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-433-    getRecordWriter(taskContext, baseOutputPath).write(key, value);
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-434-  }
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-435-
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-436-  /**
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-437-   * Write key value to an output file name.
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-438-   * 
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-439-   * Gets the record writer from job's output format.  
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-440-   * Job's output format should be a FileOutputFormat.
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-441-   * 
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-442-   * @param key       the key
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-443-   * @param value     the value
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-444-   * @param baseOutputPath base-output path to write the record to.
--
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java:448:  public void write(KEYOUT key, VALUEOUT value, String baseOutputPath) 
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-449-      throws IOException, InterruptedException {
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-450-    checkBaseOutputPath(baseOutputPath);
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-451-    if (jobOutputFormatContext == null) {
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-452-      jobOutputFormatContext = 
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-453-        new TaskAttemptContextImpl(context.getConfiguration(), 
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-454-                                   context.getTaskAttemptID(),
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-455-                                   new WrappedStatusReporter(context));
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-456-    }
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-457-    getRecordWriter(jobOutputFormatContext, baseOutputPath).write(key, value);
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-458-  }
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-459-
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-460-  // by being synchronized MultipleOutputTask can be use with a
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-461-  // MultithreadedMapper.
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-462-  @SuppressWarnings("unchecked")
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-463-  private synchronized RecordWriter getRecordWriter(
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-464-      TaskAttemptContext taskContext, String baseFileName) 
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-465-      throws IOException, InterruptedException {
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-466-    
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-467-    // look for record-writer in the cache
java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java-468-    RecordWriter writer = recordWriters.get(baseFileName);
--
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java:76:        public void write(WritableComparable<?> key, Writable value)
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-77-            throws IOException {
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-78-          out.append(key, value);
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-79-        }
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-80-
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-81-        public void close(TaskAttemptContext context) throws IOException { 
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-82-          out.close();
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-83-        }
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-84-      };
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-85-  }
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-86-
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-87-  /** Open the output generated by this format. */
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-88-  public static MapFile.Reader[] getReaders(Path dir,
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-89-      Configuration conf) throws IOException {
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-90-    FileSystem fs = dir.getFileSystem(conf);
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-91-    Path[] names = FileUtil.stat2Paths(fs.listStatus(dir));
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-92-
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-93-    // sort names, so that hash partitioning works
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-94-    Arrays.sort(names);
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-95-    
java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java-96-    MapFile.Reader[] parts = new MapFile.Reader[names.length];
--
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java:103:    public void write(KEYOUT key, VALUEOUT value) throws IOException,
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-104-        InterruptedException {
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-105-      reduceContext.write(key, value);
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-106-    }
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-107-
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-108-    @Override
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-109-    public String getStatus() {
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-110-      return reduceContext.getStatus();
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-111-    }
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-112-
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-113-    @Override
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-114-    public TaskAttemptID getTaskAttemptID() {
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-115-      return reduceContext.getTaskAttemptID();
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-116-    }
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-117-
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-118-    @Override
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-119-    public void setStatus(String msg) {
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-120-      reduceContext.setStatus(msg);
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-121-    }
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-122-
java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java-123-    @Override
--
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java:249:    public void write(KEYOUT key, VALUEOUT value) throws IOException,
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-250-        InterruptedException {
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-251-      if (outputQueue != null) {
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-252-        writeToQueue(key, value);
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-253-      } else {
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-254-        outputContext.write(key, value);
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-255-      }
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-256-    }
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-257-
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-258-    @SuppressWarnings("unchecked")
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-259-    private void writeToQueue(KEYOUT key, VALUEOUT value) throws IOException,
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-260-        InterruptedException {
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-261-      this.keyout = (KEYOUT) ReflectionUtils.newInstance(keyClass, conf);
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-262-      this.valueout = (VALUEOUT) ReflectionUtils.newInstance(valueClass, conf);
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-263-      ReflectionUtils.copy(conf, key, this.keyout);
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-264-      ReflectionUtils.copy(conf, value, this.valueout);
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-265-
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-266-      // wait to write output to queuue
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-267-      outputQueue.enqueue(new KeyValuePair<KEYOUT, VALUEOUT>(keyout, valueout));
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-268-    }
java/org/apache/hadoop/mapreduce/lib/chain/Chain.java-269-
--
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java:108:  public void write(KEYOUT key, VALUEOUT value) throws IOException,
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-109-      InterruptedException {
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-110-    output.write(key, value);
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-111-  }
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-112-
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-113-  @Override
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-114-  public String getStatus() {
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-115-    return base.getStatus();
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-116-  }
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-117-
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-118-  @Override
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-119-  public TaskAttemptID getTaskAttemptID() {
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-120-    return base.getTaskAttemptID();
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-121-  }
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-122-
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-123-  @Override
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-124-  public void setStatus(String msg) {
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-125-    base.setStatus(msg);
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-126-  }
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-127-
java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java-128-  @Override
--
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java:101:  public void write(KEYOUT key, VALUEOUT value) throws IOException,
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-102-      InterruptedException {
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-103-    rw.write(key, value);
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-104-  }
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-105-
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-106-  @Override
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-107-  public String getStatus() {
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-108-    return base.getStatus();
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-109-  }
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-110-
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-111-  @Override
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-112-  public TaskAttemptID getTaskAttemptID() {
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-113-    return base.getTaskAttemptID();
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-114-  }
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-115-
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-116-  @Override
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-117-  public void setStatus(String msg) {
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-118-    base.setStatus(msg);
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-119-  }
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-120-
java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java-121-  @Override
--
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java:116:    public void write(K key, V value) throws IOException {
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-117-      try {
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-118-        key.write(statement);
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-119-        statement.addBatch();
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-120-      } catch (SQLException e) {
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-121-        e.printStackTrace();
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-122-      }
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-123-    }
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-124-  }
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-125-
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-126-  /**
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-127-   * Constructs the query used as the prepared statement to insert data.
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-128-   * 
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-129-   * @param table
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-130-   *          the table to insert into
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-131-   * @param fieldNames
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-132-   *          the fields to insert into. If field names are unknown, supply an
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-133-   *          array of nulls.
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-134-   */
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-135-  public String constructQuery(String table, String[] fieldNames) {
java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java-136-    if(fieldNames == null) {
--
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java:111:    public void write(DataOutput output) throws IOException {
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-112-      Text.writeString(output, this.lowerBoundClause);
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-113-      Text.writeString(output, this.upperBoundClause);
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-114-    }
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-115-
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-116-    public String getLowerClause() {
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-117-      return lowerBoundClause;
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-118-    }
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-119-
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-120-    public String getUpperClause() {
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-121-      return upperBoundClause;
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-122-    }
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-123-  }
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-124-
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-125-  /**
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-126-   * @return the DBSplitter implementation to use to divide the table/query into InputSplits.
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-127-   */
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-128-  protected DBSplitter getSplitter(int sqlDataType) {
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-129-    switch (sqlDataType) {
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-130-    case Types.NUMERIC:
java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java-131-    case Types.DECIMAL:
--
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java:55: *   public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-56- *     out.writeInt(counter);
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-57- *     out.writeLong(timestamp);
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-58- *   }
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-59- *       
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-60- *   //Writable#readFields() implementation
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-61- *   public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-62- *     counter = in.readInt();
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-63- *     timestamp = in.readLong();
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-64- *   }
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-65- *       
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java:66: *   public void write(PreparedStatement statement) throws SQLException {
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-67- *     statement.setInt(1, counter);
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-68- *     statement.setLong(2, timestamp);
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-69- *   }
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-70- *       
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-71- *   public void readFields(ResultSet resultSet) throws SQLException {
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-72- *     counter = resultSet.getInt(1);
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-73- *     timestamp = resultSet.getLong(2);
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-74- *   } 
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-75- * }
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-76- * </pre></p>
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-77- */
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-78-@InterfaceAudience.Public
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-79-@InterfaceStability.Stable
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-80-public interface DBWritable {
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-81-
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-82-  /**
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-83-   * Sets the fields of the object in the {@link PreparedStatement}.
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-84-   * @param statement the statement that the fields are put into.
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-85-   * @throws SQLException
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-86-   */
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java:87:	public void write(PreparedStatement statement) throws SQLException;
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-88-	
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-89-	/**
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-90-	 * Reads the fields of the object from the {@link ResultSet}. 
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-91-	 * @param resultSet the {@link ResultSet} to get the fields from.
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-92-	 * @throws SQLException
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-93-	 */
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-94-	public void readFields(ResultSet resultSet) throws SQLException ; 
java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java-95-}
--
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java:76:    public void write(DataOutput out) throws IOException { }
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-77-    @Override
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java:78:    public void write(PreparedStatement arg0) throws SQLException { }
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-79-  }
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-80-  
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-81-  /**
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-82-   * A InputSplit that spans a set of rows
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-83-   */
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-84-  @InterfaceStability.Evolving
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-85-  public static class DBInputSplit extends InputSplit implements Writable {
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-86-
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-87-    private long end = 0;
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-88-    private long start = 0;
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-89-
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-90-    /**
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-91-     * Default Constructor
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-92-     */
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-93-    public DBInputSplit() {
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-94-    }
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-95-
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-96-    /**
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-97-     * Convenience Constructor
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-98-     * @param start the index of the first row to select
--
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java:140:    public void write(DataOutput output) throws IOException {
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-141-      output.writeLong(start);
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-142-      output.writeLong(end);
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-143-    }
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-144-  }
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-145-
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-146-  protected String conditions;
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-147-
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-148-  protected Connection connection;
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-149-
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-150-  protected String tableName;
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-151-
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-152-  protected String[] fieldNames;
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-153-
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-154-  protected DBConfiguration dbConf;
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-155-
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-156-  /** {@inheritDoc} */
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-157-  public void setConf(Configuration conf) {
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-158-
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-159-    dbConf = new DBConfiguration(conf);
java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java-160-
--
java/org/apache/hadoop/mapreduce/ID.java:90:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/ID.java-91-    out.writeInt(id);
java/org/apache/hadoop/mapreduce/ID.java-92-  }
java/org/apache/hadoop/mapreduce/ID.java-93-  
java/org/apache/hadoop/mapreduce/ID.java-94-}
--
java/org/apache/hadoop/mapreduce/QueueInfo.java:213:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/QueueInfo.java-214-    Text.writeString(out, queueName);
java/org/apache/hadoop/mapreduce/QueueInfo.java-215-    WritableUtils.writeEnum(out, queueState);
java/org/apache/hadoop/mapreduce/QueueInfo.java-216-    
java/org/apache/hadoop/mapreduce/QueueInfo.java-217-    if(schedulingInfo!= null) {
java/org/apache/hadoop/mapreduce/QueueInfo.java-218-      Text.writeString(out, schedulingInfo);
java/org/apache/hadoop/mapreduce/QueueInfo.java-219-    }else {
java/org/apache/hadoop/mapreduce/QueueInfo.java-220-      Text.writeString(out, "N/A");
java/org/apache/hadoop/mapreduce/QueueInfo.java-221-    }
java/org/apache/hadoop/mapreduce/QueueInfo.java-222-    out.writeInt(stats.length);
java/org/apache/hadoop/mapreduce/QueueInfo.java-223-    for (JobStatus stat : stats) {
java/org/apache/hadoop/mapreduce/QueueInfo.java-224-      stat.write(out);
java/org/apache/hadoop/mapreduce/QueueInfo.java-225-    }
java/org/apache/hadoop/mapreduce/QueueInfo.java-226-    out.writeInt(children.size());
java/org/apache/hadoop/mapreduce/QueueInfo.java-227-    for(QueueInfo childQueueInfo : children) {
java/org/apache/hadoop/mapreduce/QueueInfo.java-228-      childQueueInfo.write(out);
java/org/apache/hadoop/mapreduce/QueueInfo.java-229-    }
java/org/apache/hadoop/mapreduce/QueueInfo.java-230-  }
java/org/apache/hadoop/mapreduce/QueueInfo.java-231-}
--
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java:57:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-58-    counters.write(out);
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-59-    WritableUtils.writeVLong(out, partialOutput.size());
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-60-    for (Path p : partialOutput) {
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-61-      Text.writeString(out, p.toString());
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-62-    }
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-63-    rawId.write(out);
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-64-  }
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-65-
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-66-  @Override
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-67-  public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-68-    partialOutput.clear();
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-69-    counters.readFields(in);
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-70-    long numPout = WritableUtils.readVLong(in);
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-71-    for (int i = 0; i < numPout; i++) {
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-72-      partialOutput.add(new Path(Text.readString(in)));
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-73-    }
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-74-    rawId.readFields(in);
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-75-  }
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-76-
java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java-77-  @Override
--
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java:52:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-53-    Text.writeString(out, path.toString());
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-54-  }
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-55-
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-56-  @Override
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-57-  public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-58-    this.path = new Path(Text.readString(in));
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-59-  }
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-60-
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-61-  @Override
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-62-  public boolean equals(Object other) {
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-63-    return other instanceof FSCheckpointID
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-64-      && path.equals(((FSCheckpointID)other).path);
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-65-  }
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-66-
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-67-  @Override
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-68-  public int hashCode() {
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-69-    return path.hashCode();
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-70-  }
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-71-
java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java-72-}
--
java/org/apache/hadoop/mapreduce/TaskTrackerInfo.java:101:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/TaskTrackerInfo.java-102-    Text.writeString(out, name);
java/org/apache/hadoop/mapreduce/TaskTrackerInfo.java-103-    out.writeBoolean(isBlacklisted);
java/org/apache/hadoop/mapreduce/TaskTrackerInfo.java-104-    Text.writeString(out, reasonForBlacklist);
java/org/apache/hadoop/mapreduce/TaskTrackerInfo.java-105-    Text.writeString(out, blacklistReport);
java/org/apache/hadoop/mapreduce/TaskTrackerInfo.java-106-  }
java/org/apache/hadoop/mapreduce/TaskTrackerInfo.java-107-
java/org/apache/hadoop/mapreduce/TaskTrackerInfo.java-108-}
--
java/org/apache/hadoop/mapreduce/ClusterMetrics.java:244:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/ClusterMetrics.java-245-    out.writeInt(runningMaps);
java/org/apache/hadoop/mapreduce/ClusterMetrics.java-246-    out.writeInt(runningReduces);
java/org/apache/hadoop/mapreduce/ClusterMetrics.java-247-    out.writeInt(occupiedMapSlots);
java/org/apache/hadoop/mapreduce/ClusterMetrics.java-248-    out.writeInt(occupiedReduceSlots);
java/org/apache/hadoop/mapreduce/ClusterMetrics.java-249-    out.writeInt(reservedMapSlots);
java/org/apache/hadoop/mapreduce/ClusterMetrics.java-250-    out.writeInt(reservedReduceSlots);
java/org/apache/hadoop/mapreduce/ClusterMetrics.java-251-    out.writeInt(totalMapSlots);
java/org/apache/hadoop/mapreduce/ClusterMetrics.java-252-    out.writeInt(totalReduceSlots);
java/org/apache/hadoop/mapreduce/ClusterMetrics.java-253-    out.writeInt(totalJobSubmissions);
java/org/apache/hadoop/mapreduce/ClusterMetrics.java-254-    out.writeInt(numTrackers);
java/org/apache/hadoop/mapreduce/ClusterMetrics.java-255-    out.writeInt(numBlacklistedTrackers);
java/org/apache/hadoop/mapreduce/ClusterMetrics.java-256-    out.writeInt(numGraylistedTrackers);
java/org/apache/hadoop/mapreduce/ClusterMetrics.java-257-    out.writeInt(numDecommissionedTrackers);
java/org/apache/hadoop/mapreduce/ClusterMetrics.java-258-  }
java/org/apache/hadoop/mapreduce/ClusterMetrics.java-259-
java/org/apache/hadoop/mapreduce/ClusterMetrics.java-260-}
--
java/org/apache/hadoop/mapreduce/task/reduce/ShuffleHeader.java:72:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/task/reduce/ShuffleHeader.java-73-    Text.writeString(out, mapId);
java/org/apache/hadoop/mapreduce/task/reduce/ShuffleHeader.java-74-    WritableUtils.writeVLong(out, compressedLength);
java/org/apache/hadoop/mapreduce/task/reduce/ShuffleHeader.java-75-    WritableUtils.writeVLong(out, uncompressedLength);
java/org/apache/hadoop/mapreduce/task/reduce/ShuffleHeader.java-76-    WritableUtils.writeVInt(out, forReduce);
java/org/apache/hadoop/mapreduce/task/reduce/ShuffleHeader.java-77-  }
java/org/apache/hadoop/mapreduce/task/reduce/ShuffleHeader.java-78-}
--
java/org/apache/hadoop/mapreduce/task/TaskInputOutputContextImpl.java:87:  public void write(KEYOUT key, VALUEOUT value
java/org/apache/hadoop/mapreduce/task/TaskInputOutputContextImpl.java-88-                    ) throws IOException, InterruptedException {
java/org/apache/hadoop/mapreduce/task/TaskInputOutputContextImpl.java-89-    output.write(key, value);
java/org/apache/hadoop/mapreduce/task/TaskInputOutputContextImpl.java-90-  }
java/org/apache/hadoop/mapreduce/task/TaskInputOutputContextImpl.java-91-
java/org/apache/hadoop/mapreduce/task/TaskInputOutputContextImpl.java-92-  public OutputCommitter getOutputCommitter() {
java/org/apache/hadoop/mapreduce/task/TaskInputOutputContextImpl.java-93-    return committer;
java/org/apache/hadoop/mapreduce/task/TaskInputOutputContextImpl.java-94-  }
java/org/apache/hadoop/mapreduce/task/TaskInputOutputContextImpl.java-95-}
--
java/org/apache/hadoop/mapreduce/TaskAttemptID.java:143:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-144-    super.write(out);
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-145-    taskId.write(out);
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-146-  }
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-147-
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-148-  @Override
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-149-  public int hashCode() {
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-150-    return taskId.hashCode() * 5 + id;
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-151-  }
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-152-  
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-153-  /**Compare TaskIds by first tipIds, then by task numbers. */
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-154-  @Override
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-155-  public int compareTo(ID o) {
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-156-    TaskAttemptID that = (TaskAttemptID)o;
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-157-    int tipComp = this.taskId.compareTo(that.taskId);
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-158-    if(tipComp == 0) {
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-159-      return this.id - that.id;
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-160-    }
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-161-    else return tipComp;
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-162-  }
java/org/apache/hadoop/mapreduce/TaskAttemptID.java-163-  @Override
--
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java:62:  synchronized void write(HistoryEvent event) throws IOException { 
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-63-    Event wrapper = new Event();
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-64-    wrapper.type = event.getEventType();
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-65-    wrapper.event = event.getDatum();
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-66-    writer.write(wrapper, encoder);
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-67-    encoder.flush();
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-68-    out.writeBytes("\n");
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-69-  }
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-70-  
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-71-  void flush() throws IOException {
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-72-    encoder.flush();
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-73-    out.flush();
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-74-    out.hflush();
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-75-  }
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-76-
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-77-  void close() throws IOException {
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-78-    try {
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-79-      encoder.flush();
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-80-      out.close();
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-81-      out = null;
java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java-82-    } finally {
--
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java:158:  public synchronized void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-159-    Text.writeString(out, displayName);
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-160-    WritableUtils.writeVInt(out, counters.size());
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-161-    for(Counter counter: counters.values()) {
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-162-      counter.write(out);
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-163-    }
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-164-  }
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-165-
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-166-  @Override
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-167-  public synchronized void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-168-    displayName = StringInterner.weakIntern(Text.readString(in));
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-169-    counters.clear();
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-170-    int size = WritableUtils.readVInt(in);
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-171-    for (int i = 0; i < size; i++) {
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-172-      T counter = newCounter();
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-173-      counter.readFields(in);
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-174-      counters.put(counter.getName(), counter);
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-175-      limits.incrCounters();
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-176-    }
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-177-  }
java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java-178-
--
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java:259:  public synchronized void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-260-    WritableUtils.writeVInt(out, groupFactory.version());
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-261-    WritableUtils.writeVInt(out, fgroups.size());  // framework groups first
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-262-    for (G group : fgroups.values()) {
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-263-      if (group.getUnderlyingGroup() instanceof FrameworkCounterGroup<?, ?>) {
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-264-        WritableUtils.writeVInt(out, GroupType.FRAMEWORK.ordinal());
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-265-        WritableUtils.writeVInt(out, getFrameworkGroupId(group.getName()));
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-266-        group.write(out);
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-267-      } else if (group.getUnderlyingGroup() instanceof FileSystemCounterGroup<?>) {
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-268-        WritableUtils.writeVInt(out, GroupType.FILESYSTEM.ordinal());
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-269-        group.write(out);
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-270-      }
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-271-    }
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-272-    if (writeAllCounters) {
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-273-      WritableUtils.writeVInt(out, groups.size());
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-274-      for (G group : groups.values()) {
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-275-        Text.writeString(out, group.getName());
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-276-        group.write(out);
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-277-      }
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-278-    } else {
java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java-279-      WritableUtils.writeVInt(out, 0);
--
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java:118:    public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-119-      assert false : "shouldn't be called";
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-120-    }
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-121-
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-122-    @Override
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-123-    public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-124-      assert false : "shouldn't be called";
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-125-    }
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-126-
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-127-    @Override
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-128-    public Counter getUnderlyingCounter() {
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-129-      return this;
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-130-    }
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-131-  }
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-132-
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-133-  @Override
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-134-  public String getName() {
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-135-    return FileSystemCounter.class.getName();
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-136-  }
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-137-
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-138-  @Override
--
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java:264:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-265-    WritableUtils.writeVInt(out, map.size()); // #scheme
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-266-    for (Map.Entry<String, Object[]> entry : map.entrySet()) {
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-267-      WritableUtils.writeString(out, entry.getKey()); // scheme
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-268-      // #counter for the above scheme
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-269-      WritableUtils.writeVInt(out, numSetCounters(entry.getValue()));
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-270-      for (Object counter : entry.getValue()) {
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-271-        if (counter == null) continue;
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-272-        @SuppressWarnings("unchecked")
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-273-        FSCounter c = (FSCounter) ((Counter)counter).getUnderlyingCounter();
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-274-        WritableUtils.writeVInt(out, c.key.ordinal());  // key
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-275-        WritableUtils.writeVLong(out, c.getValue());    // value
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-276-      }
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-277-    }
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-278-  }
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-279-
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-280-  private int numSetCounters(Object[] counters) {
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-281-    int n = 0;
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-282-    for (Object counter : counters) if (counter != null) ++n;
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-283-    return n;
java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java-284-  }
--
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java:107:    public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-108-      assert false : "shouldn't be called";
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-109-    }
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-110-
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-111-    @Override
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-112-    public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-113-      assert false : "shouldn't be called";
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-114-    }
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-115-
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-116-    @Override
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-117-    public Counter getUnderlyingCounter() {
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-118-      return this;
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-119-    }
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-120-  }
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-121-
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-122-  @SuppressWarnings("unchecked")
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-123-  public FrameworkCounterGroup(Class<T> enumClass) {
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-124-    this.enumClass = enumClass;
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-125-    T[] enums = enumClass.getEnumConstants();
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-126-    counters = new Object[enums.length];
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-127-  }
--
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java:227:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-228-    WritableUtils.writeVInt(out, size());
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-229-    for (int i = 0; i < counters.length; ++i) {
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-230-      Counter counter = (C) counters[i];
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-231-      if (counter != null) {
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-232-        WritableUtils.writeVInt(out, i);
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-233-        WritableUtils.writeVLong(out, counter.getValue());
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-234-      }
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-235-    }
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-236-  }
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-237-
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-238-  @Override
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-239-  public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-240-    clear();
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-241-    int len = WritableUtils.readVInt(in);
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-242-    T[] enums = enumClass.getEnumConstants();
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-243-    for (int i = 0; i < len; ++i) {
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-244-      int ord = WritableUtils.readVInt(in);
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-245-      Counter counter = newCounter(enums[ord]);
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-246-      counter.setValue(WritableUtils.readVLong(in));
java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java-247-      counters[ord] = counter;
--
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java:73:  public synchronized void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-74-    Text.writeString(out, name);
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-75-    boolean distinctDisplayName = ! name.equals(displayName);
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-76-    out.writeBoolean(distinctDisplayName);
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-77-    if (distinctDisplayName) {
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-78-      Text.writeString(out, displayName);
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-79-    }
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-80-    WritableUtils.writeVLong(out, value);
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-81-  }
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-82-
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-83-  @Override
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-84-  public synchronized String getName() {
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-85-    return name;
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-86-  }
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-87-
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-88-  @Override
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-89-  public synchronized String getDisplayName() {
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-90-    return displayName;
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-91-  }
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-92-
java/org/apache/hadoop/mapreduce/counters/GenericCounter.java-93-  @Override
--
java/org/apache/hadoop/mapreduce/security/token/JobTokenIdentifier.java:87:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/security/token/JobTokenIdentifier.java-88-    jobid.write(out);
java/org/apache/hadoop/mapreduce/security/token/JobTokenIdentifier.java-89-  }
java/org/apache/hadoop/mapreduce/security/token/JobTokenIdentifier.java-90-
java/org/apache/hadoop/mapreduce/security/token/JobTokenIdentifier.java-91-  @InterfaceAudience.Private
java/org/apache/hadoop/mapreduce/security/token/JobTokenIdentifier.java-92-  public static class Renewer extends Token.TrivialRenewer {
java/org/apache/hadoop/mapreduce/security/token/JobTokenIdentifier.java-93-    @Override
java/org/apache/hadoop/mapreduce/security/token/JobTokenIdentifier.java-94-    protected Text getKind() {
java/org/apache/hadoop/mapreduce/security/token/JobTokenIdentifier.java-95-      return KIND_NAME;
java/org/apache/hadoop/mapreduce/security/token/JobTokenIdentifier.java-96-    }
java/org/apache/hadoop/mapreduce/security/token/JobTokenIdentifier.java-97-  }
java/org/apache/hadoop/mapreduce/security/token/JobTokenIdentifier.java-98-}
--
java/org/apache/hadoop/mapreduce/JobStatus.java:424:  public synchronized void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/JobStatus.java-425-    jobid.write(out);
java/org/apache/hadoop/mapreduce/JobStatus.java-426-    out.writeFloat(setupProgress);
java/org/apache/hadoop/mapreduce/JobStatus.java-427-    out.writeFloat(mapProgress);
java/org/apache/hadoop/mapreduce/JobStatus.java-428-    out.writeFloat(reduceProgress);
java/org/apache/hadoop/mapreduce/JobStatus.java-429-    out.writeFloat(cleanupProgress);
java/org/apache/hadoop/mapreduce/JobStatus.java-430-    WritableUtils.writeEnum(out, runState);
java/org/apache/hadoop/mapreduce/JobStatus.java-431-    out.writeLong(startTime);
java/org/apache/hadoop/mapreduce/JobStatus.java-432-    Text.writeString(out, user);
java/org/apache/hadoop/mapreduce/JobStatus.java-433-    WritableUtils.writeEnum(out, priority);
java/org/apache/hadoop/mapreduce/JobStatus.java-434-    Text.writeString(out, schedulingInfo);
java/org/apache/hadoop/mapreduce/JobStatus.java-435-    out.writeLong(finishTime);
java/org/apache/hadoop/mapreduce/JobStatus.java-436-    out.writeBoolean(isRetired);
java/org/apache/hadoop/mapreduce/JobStatus.java-437-    Text.writeString(out, historyFile);
java/org/apache/hadoop/mapreduce/JobStatus.java-438-    Text.writeString(out, jobName);
java/org/apache/hadoop/mapreduce/JobStatus.java-439-    Text.writeString(out, trackingUrl);
java/org/apache/hadoop/mapreduce/JobStatus.java-440-    Text.writeString(out, jobFile);
java/org/apache/hadoop/mapreduce/JobStatus.java-441-    out.writeBoolean(isUber);
java/org/apache/hadoop/mapreduce/JobStatus.java-442-
java/org/apache/hadoop/mapreduce/JobStatus.java-443-    // Serialize the job's ACLs
java/org/apache/hadoop/mapreduce/JobStatus.java-444-    out.writeInt(jobACLs.size());
--
java/org/apache/hadoop/mapreduce/QueueAclsInfo.java:91:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/QueueAclsInfo.java-92-    Text.writeString(out, queueName);
java/org/apache/hadoop/mapreduce/QueueAclsInfo.java-93-    WritableUtils.writeStringArray(out, operations);
java/org/apache/hadoop/mapreduce/QueueAclsInfo.java-94-  }
java/org/apache/hadoop/mapreduce/QueueAclsInfo.java-95-}
--
java/org/apache/hadoop/mapreduce/JobID.java:134:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/JobID.java-135-    super.write(out);
java/org/apache/hadoop/mapreduce/JobID.java-136-    jtIdentifier.write(out);
java/org/apache/hadoop/mapreduce/JobID.java-137-  }
java/org/apache/hadoop/mapreduce/JobID.java-138-  
java/org/apache/hadoop/mapreduce/JobID.java-139-  /** Construct a JobId object from given string 
java/org/apache/hadoop/mapreduce/JobID.java-140-   * @return constructed JobId object or null if the given String is null
java/org/apache/hadoop/mapreduce/JobID.java-141-   * @throws IllegalArgumentException if the given string is malformed
java/org/apache/hadoop/mapreduce/JobID.java-142-   */
java/org/apache/hadoop/mapreduce/JobID.java-143-  public static JobID forName(String str) throws IllegalArgumentException {
java/org/apache/hadoop/mapreduce/JobID.java-144-    if(str == null)
java/org/apache/hadoop/mapreduce/JobID.java-145-      return null;
java/org/apache/hadoop/mapreduce/JobID.java-146-    try {
java/org/apache/hadoop/mapreduce/JobID.java-147-      String[] parts = str.split("_");
java/org/apache/hadoop/mapreduce/JobID.java-148-      if(parts.length == 3) {
java/org/apache/hadoop/mapreduce/JobID.java-149-        if(parts[0].equals(JOB)) {
java/org/apache/hadoop/mapreduce/JobID.java-150-          return new org.apache.hadoop.mapred.JobID(parts[1], 
java/org/apache/hadoop/mapreduce/JobID.java-151-                                                    Integer.parseInt(parts[2]));
java/org/apache/hadoop/mapreduce/JobID.java-152-        }
java/org/apache/hadoop/mapreduce/JobID.java-153-      }
java/org/apache/hadoop/mapreduce/JobID.java-154-    }catch (Exception ex) {//fall below
--
java/org/apache/hadoop/mapreduce/TaskID.java:196:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/TaskID.java-197-    super.write(out);
java/org/apache/hadoop/mapreduce/TaskID.java-198-    jobId.write(out);
java/org/apache/hadoop/mapreduce/TaskID.java-199-    WritableUtils.writeEnum(out, type);
java/org/apache/hadoop/mapreduce/TaskID.java-200-  }
java/org/apache/hadoop/mapreduce/TaskID.java-201-  
java/org/apache/hadoop/mapreduce/TaskID.java-202-  /** Construct a TaskID object from given string 
java/org/apache/hadoop/mapreduce/TaskID.java-203-   * @return constructed TaskID object or null if the given String is null
java/org/apache/hadoop/mapreduce/TaskID.java-204-   * @throws IllegalArgumentException if the given string is malformed
java/org/apache/hadoop/mapreduce/TaskID.java-205-   */
java/org/apache/hadoop/mapreduce/TaskID.java-206-  public static TaskID forName(String str) 
java/org/apache/hadoop/mapreduce/TaskID.java-207-    throws IllegalArgumentException {
java/org/apache/hadoop/mapreduce/TaskID.java-208-    if(str == null)
java/org/apache/hadoop/mapreduce/TaskID.java-209-      return null;
java/org/apache/hadoop/mapreduce/TaskID.java-210-    String exceptionMsg = null;
java/org/apache/hadoop/mapreduce/TaskID.java-211-    try {
java/org/apache/hadoop/mapreduce/TaskID.java-212-      String[] parts = str.split("_");
java/org/apache/hadoop/mapreduce/TaskID.java-213-      if(parts.length == 5) {
java/org/apache/hadoop/mapreduce/TaskID.java-214-        if(parts[0].equals(TASK)) {
java/org/apache/hadoop/mapreduce/TaskID.java-215-          String type = parts[3];
java/org/apache/hadoop/mapreduce/TaskID.java-216-          TaskType t = CharTaskTypeMaps.getTaskType(type.charAt(0));
--
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java:201:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-202-    taskId.write(out); 
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-203-    WritableUtils.writeVInt(out, idWithinJob);
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-204-    out.writeBoolean(isMap);
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-205-    WritableUtils.writeEnum(out, status); 
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-206-    WritableUtils.writeString(out, taskTrackerHttp);
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-207-    WritableUtils.writeVInt(out, taskRunTime);
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-208-    WritableUtils.writeVInt(out, eventId);
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-209-  }
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-210-  
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-211-  public void readFields(DataInput in) throws IOException {
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-212-    taskId.readFields(in); 
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-213-    idWithinJob = WritableUtils.readVInt(in);
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-214-    isMap = in.readBoolean();
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-215-    status = WritableUtils.readEnum(in, Status.class);
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-216-    taskTrackerHttp = WritableUtils.readString(in);
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-217-    taskRunTime = WritableUtils.readVInt(in);
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-218-    eventId = WritableUtils.readVInt(in);
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-219-  }
java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java-220-}
--
java/org/apache/hadoop/mapreduce/TaskReport.java:196:  public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/TaskReport.java-197-    taskid.write(out);
java/org/apache/hadoop/mapreduce/TaskReport.java-198-    out.writeFloat(progress);
java/org/apache/hadoop/mapreduce/TaskReport.java-199-    Text.writeString(out, state);
java/org/apache/hadoop/mapreduce/TaskReport.java-200-    out.writeLong(startTime);
java/org/apache/hadoop/mapreduce/TaskReport.java-201-    out.writeLong(finishTime);
java/org/apache/hadoop/mapreduce/TaskReport.java-202-    WritableUtils.writeStringArray(out, diagnostics);
java/org/apache/hadoop/mapreduce/TaskReport.java-203-    counters.write(out);
java/org/apache/hadoop/mapreduce/TaskReport.java-204-    WritableUtils.writeEnum(out, currentStatus);
java/org/apache/hadoop/mapreduce/TaskReport.java-205-    if (currentStatus == TIPStatus.RUNNING) {
java/org/apache/hadoop/mapreduce/TaskReport.java-206-      WritableUtils.writeVInt(out, runningAttempts.size());
java/org/apache/hadoop/mapreduce/TaskReport.java-207-      TaskAttemptID t[] = new TaskAttemptID[0];
java/org/apache/hadoop/mapreduce/TaskReport.java-208-      t = runningAttempts.toArray(t);
java/org/apache/hadoop/mapreduce/TaskReport.java-209-      for (int i = 0; i < t.length; i++) {
java/org/apache/hadoop/mapreduce/TaskReport.java-210-        t[i].write(out);
java/org/apache/hadoop/mapreduce/TaskReport.java-211-      }
java/org/apache/hadoop/mapreduce/TaskReport.java-212-    } else if (currentStatus == TIPStatus.COMPLETE) {
java/org/apache/hadoop/mapreduce/TaskReport.java-213-      successfulAttempt.write(out);
java/org/apache/hadoop/mapreduce/TaskReport.java-214-    }
java/org/apache/hadoop/mapreduce/TaskReport.java-215-  }
java/org/apache/hadoop/mapreduce/TaskReport.java-216-
--
java/org/apache/hadoop/mapreduce/RecordWriter.java:46:  public abstract void write(K key, V value
java/org/apache/hadoop/mapreduce/RecordWriter.java-47-                             ) throws IOException, InterruptedException;
java/org/apache/hadoop/mapreduce/RecordWriter.java-48-
java/org/apache/hadoop/mapreduce/RecordWriter.java-49-  /** 
java/org/apache/hadoop/mapreduce/RecordWriter.java-50-   * Close this <code>RecordWriter</code> to future operations.
java/org/apache/hadoop/mapreduce/RecordWriter.java-51-   * 
java/org/apache/hadoop/mapreduce/RecordWriter.java-52-   * @param context the context of the task
java/org/apache/hadoop/mapreduce/RecordWriter.java-53-   * @throws IOException
java/org/apache/hadoop/mapreduce/RecordWriter.java-54-   */ 
java/org/apache/hadoop/mapreduce/RecordWriter.java-55-  public abstract void close(TaskAttemptContext context
java/org/apache/hadoop/mapreduce/RecordWriter.java-56-                             ) throws IOException, InterruptedException;
java/org/apache/hadoop/mapreduce/RecordWriter.java-57-}
--
java/org/apache/hadoop/mapreduce/split/JobSplit.java:122:    public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/split/JobSplit.java-123-      WritableUtils.writeVInt(out, locations.length);
java/org/apache/hadoop/mapreduce/split/JobSplit.java-124-      for (int i = 0; i < locations.length; i++) {
java/org/apache/hadoop/mapreduce/split/JobSplit.java-125-        Text.writeString(out, locations[i]);
java/org/apache/hadoop/mapreduce/split/JobSplit.java-126-      }
java/org/apache/hadoop/mapreduce/split/JobSplit.java-127-      WritableUtils.writeVLong(out, startOffset);
java/org/apache/hadoop/mapreduce/split/JobSplit.java-128-      WritableUtils.writeVLong(out, inputDataLength);
java/org/apache/hadoop/mapreduce/split/JobSplit.java-129-    }
java/org/apache/hadoop/mapreduce/split/JobSplit.java-130-    
java/org/apache/hadoop/mapreduce/split/JobSplit.java-131-    @Override
java/org/apache/hadoop/mapreduce/split/JobSplit.java-132-    public String toString() {
java/org/apache/hadoop/mapreduce/split/JobSplit.java-133-      StringBuffer buf = new StringBuffer();
java/org/apache/hadoop/mapreduce/split/JobSplit.java-134-      buf.append("data-size : " + inputDataLength + "\n");
java/org/apache/hadoop/mapreduce/split/JobSplit.java-135-      buf.append("start-offset : " + startOffset + "\n");
java/org/apache/hadoop/mapreduce/split/JobSplit.java-136-      buf.append("locations : " + "\n");
java/org/apache/hadoop/mapreduce/split/JobSplit.java-137-      for (String loc : locations) {
java/org/apache/hadoop/mapreduce/split/JobSplit.java-138-        buf.append("  " + loc + "\n");
java/org/apache/hadoop/mapreduce/split/JobSplit.java-139-      }
java/org/apache/hadoop/mapreduce/split/JobSplit.java-140-      return buf.toString();
java/org/apache/hadoop/mapreduce/split/JobSplit.java-141-    }
java/org/apache/hadoop/mapreduce/split/JobSplit.java-142-  }
--
java/org/apache/hadoop/mapreduce/split/JobSplit.java:214:    public void write(DataOutput out) throws IOException {
java/org/apache/hadoop/mapreduce/split/JobSplit.java-215-      Text.writeString(out, splitLocation);
java/org/apache/hadoop/mapreduce/split/JobSplit.java-216-      WritableUtils.writeVLong(out, startOffset);
java/org/apache/hadoop/mapreduce/split/JobSplit.java-217-    }
java/org/apache/hadoop/mapreduce/split/JobSplit.java-218-  }
java/org/apache/hadoop/mapreduce/split/JobSplit.java-219-}
--
java/org/apache/hadoop/mapreduce/TaskInputOutputContext.java:64:  public void write(KEYOUT key, VALUEOUT value) 
java/org/apache/hadoop/mapreduce/TaskInputOutputContext.java-65-      throws IOException, InterruptedException;
java/org/apache/hadoop/mapreduce/TaskInputOutputContext.java-66-
java/org/apache/hadoop/mapreduce/TaskInputOutputContext.java-67-  /**
java/org/apache/hadoop/mapreduce/TaskInputOutputContext.java-68-   * Get the {@link OutputCommitter} for the task-attempt.
java/org/apache/hadoop/mapreduce/TaskInputOutputContext.java-69-   * @return the <code>OutputCommitter</code> for the task-attempt
java/org/apache/hadoop/mapreduce/TaskInputOutputContext.java-70-   */
java/org/apache/hadoop/mapreduce/TaskInputOutputContext.java-71-  public OutputCommitter getOutputCommitter();
java/org/apache/hadoop/mapreduce/TaskInputOutputContext.java-72-}
